{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BmDD5RbyTZn",
      "metadata": {
        "id": "-BmDD5RbyTZn"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7tUrskDXyJrq",
      "metadata": {
        "id": "7tUrskDXyJrq"
      },
      "source": [
        "# Companion Code for SOL310: Adding Agentic Aware Security Next '25\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/hselbie/Companion_SOL310_Next25_Agentic-Aware-Security/blob/main/notebook.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fhugoselbie%2FCompanion_SOL310_Next25_Agentic-Aware-Security%2Fmain%2Fnotebook.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/hugoselbie/Companion_SOL310_Next25_Agentic-Aware-Security/main/notebook.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/hugoselbie/Companion_SOL310_Next25_Agentic-Aware-Security\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<div style=\"clear: both;\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704f4b1e-bfa2-4597-b768-8bec2e77e3fa",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "704f4b1e-bfa2-4597-b768-8bec2e77e3fa"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "REAjv_pA9vH3",
      "metadata": {
        "id": "REAjv_pA9vH3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform[agent_engines,langchain]\" \\\n",
        "    langgraph \\\n",
        "    langchain \\\n",
        "    google-cloud-aiplatform google-cloud-discoveryengine \\\n",
        "    cloudpickle==3.0.0 \\\n",
        "    \"pydantic>=2.11.2\" \\\n",
        "    requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "25521614-ad0d-4692-a3a5-eafdf37566fd",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "25521614-ad0d-4692-a3a5-eafdf37566fd"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import operator\n",
        "from copy import deepcopy\n",
        "from typing import TypedDict, Annotated, List, Literal, Optional\n",
        "from dataclasses import dataclass\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "import requests\n",
        "import vertexai\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import ToolNode, tools_condition, InjectedState\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from vertexai import agent_engines\n",
        "from vertexai.preview.reasoning_engines import LangchainAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iyWsFPbmOLI6",
      "metadata": {
        "id": "iyWsFPbmOLI6"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7480b97",
      "metadata": {},
      "source": [
        "# Project Setup\n",
        "To execute the usecases outlined in SOL310: Adding Agentic Aware Security presented at Next '25, there are a few setup parameters to go through. The case study is using vertex ai search with an associated document datastore derived from documents that are stored in GCS. \n",
        "\n",
        "To execute the example below we're expecting you to have already configured a VAIS search app with associated datastore and have documents that are stored in a known GCS bucket that you have read/write permissions on. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tx0Gjxgg-LnW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Tx0Gjxgg-LnW",
        "outputId": "271c4cc0-a9ba-407c-b90c-e4939e55e90c"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = 123456789\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID' #@param {type:\"string\"}\n",
        "BUCKET_NAME = 'YOUR_BUCKET' #@param {type:\"string\"}\n",
        "DATA_STORE_ID = 'YOUR_DATASTORE_ID'  #@param {type:\"string\"} \n",
        "SEARCH_APP = 'YOUR_SEARCH_APP'  #@param {type:\"string\"} \n",
        "LOCATION = 'global' #@param {type:\"string\"} \n",
        "API_DOMAIN = 'discoveryengine' #@param {type:\"string\"}\n",
        "\n",
        "SEARCH_URL = f'https://{API_DOMAIN}.googleapis.com/v1alpha/projects/{PROJECT_NUMBER}/locations/{LOCATION}/collections/default_collection/engines/{SEARCH_APP}/servingConfigs/default_search:search'\n",
        "LIST_DOCUMENTS_URL = f'https://{API_DOMAIN}.googleapis.com/v1/projects/{PROJECT_NUMBER}/locations/{LOCATION}/collections/default_collection/dataStores/{DATA_STORE_ID}/branches/default_branch/documents'\n",
        "CREATE_DOCUMENT_URL_TEMPLATE = f'https://{API_DOMAIN}.googleapis.com/v1/projects/{PROJECT_NUMBER}/locations/{LOCATION}/collections/default_collection/dataStores/{DATA_STORE_ID}/branches/default_branch/documents?documentId={{}}'\n",
        "DOCUMENT_URL_TEMPLATE = f'https://{API_DOMAIN}.googleapis.com/v1/projects/{PROJECT_NUMBER}/locations/{LOCATION}/collections/default_collection/dataStores/{DATA_STORE_ID}/branches/default_branch/documents/{{}}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "bf4da554-a3ce-44d8-bfba-cfdc805b368d",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "bf4da554-a3ce-44d8-bfba-cfdc805b368d"
      },
      "outputs": [],
      "source": [
        "os.environ['LOCAL'] = 'true'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "442f968e-2d02-4753-87ec-854c80fb8666",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "442f968e-2d02-4753-87ec-854c80fb8666"
      },
      "source": [
        "# Defining access control and getting permission information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "9eee5068-8425-4a1c-8296-a1209a5932d5",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "9eee5068-8425-4a1c-8296-a1209a5932d5"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Access:\n",
        "    scope: str\n",
        "    specifier: str\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return (self.scope, self.specifier) < (other.scope, other.specifier)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.scope, self.specifier))\n",
        "\n",
        "    def match(self, other):\n",
        "        if self.scope != other.scope:\n",
        "            return None\n",
        "        if self.specifier == '*':\n",
        "            return other\n",
        "        if other.specifier == '*':\n",
        "            return self\n",
        "        if self.specifier == other.specifier:\n",
        "            return self\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return f'{self.scope}_{self.specifier}'\n",
        "\n",
        "\n",
        "class Restriction:\n",
        "\n",
        "    HASH_OUTPUT_BYTES = 16\n",
        "\n",
        "    def __init__(self, access):\n",
        "        self.accesses = [access]\n",
        "\n",
        "    def update(self, access):\n",
        "        if access in self.accesses:\n",
        "            raise ValueError('this access is already present')\n",
        "        if access.scope in (x.scope for x in self.accesses):\n",
        "            raise ValueError('access with the same scope is already present')\n",
        "        self.accesses.append(access)\n",
        "        self.accesses.sort()  # we want to have a stable order\n",
        "\n",
        "    def __str__(self):\n",
        "        return ' & '.join(str(x) for x in self.accesses)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(self.accesses))  # already sorted\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.accesses == other.accesses\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "    @property\n",
        "    def hash(self):\n",
        "        string = str(self)\n",
        "        return hashlib.shake_128(string.encode()).hexdigest(self.HASH_OUTPUT_BYTES)\n",
        "\n",
        "    def match(self, other):\n",
        "        if len(self.accesses) != len(other.accesses):\n",
        "            return None\n",
        "\n",
        "        accesses = []\n",
        "        for access in self.accesses:\n",
        "            matched = [access.match(x) for x in other.accesses]\n",
        "            matched = [x for x in matched if x]\n",
        "            if not matched:\n",
        "                break\n",
        "            assert len(matched) == 1\n",
        "            accesses.extend(matched)\n",
        "\n",
        "        if not accesses:\n",
        "            return None\n",
        "        restriction = Restriction(accesses.pop())\n",
        "        while accesses:\n",
        "            restriction.update(accesses.pop())\n",
        "        return restriction\n",
        "\n",
        "    def has_wildcard(self):\n",
        "        return any(x for x in self.accesses if x.specifier == '*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "019f2f82-b0c2-461f-8898-639a1008fb13",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "019f2f82-b0c2-461f-8898-639a1008fb13"
      },
      "outputs": [],
      "source": [
        "class Principal:\n",
        "\n",
        "    def __new__(cls, id_):\n",
        "        instance = cls.instances.get(id_)\n",
        "        if instance is None:\n",
        "            instance = super().__new__(cls)\n",
        "            cls.instances[id_] = instance\n",
        "        return instance\n",
        "\n",
        "    def __init__(self, id_, *args, **kwargs):\n",
        "        if hasattr(self, 'id'):  # is initialized?\n",
        "            return\n",
        "        self.id = id_\n",
        "        self.read_restriction = None\n",
        "        self.write_restriction = None\n",
        "        self.membership = set()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.id!r})'\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.id)\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.id < other.id\n",
        "\n",
        "    @property\n",
        "    def membership_expanded(self):\n",
        "        expanded = set()\n",
        "        stack = list(self.membership)\n",
        "        while stack:\n",
        "            group = stack.pop()\n",
        "            expanded.add(group)\n",
        "            stack.extend(group.membership)\n",
        "        return expanded\n",
        "\n",
        "    def restrict_read(self, access):\n",
        "        if self.read_restriction is None:\n",
        "            self.read_restriction = Restriction(access)\n",
        "        else:\n",
        "            self.read_restriction.update(access)\n",
        "\n",
        "    def restrict_write(self, access):\n",
        "        if self.write_restriction is None:\n",
        "            self.write_restriction = Restriction(access)\n",
        "        else:\n",
        "            self.write_restriction.update(access)\n",
        "\n",
        "    @classmethod\n",
        "    def all(cls):\n",
        "        return cls.instances\n",
        "\n",
        "    @classmethod\n",
        "    def clear_all(cls):\n",
        "        cls.instances.clear()\n",
        "\n",
        "    @property\n",
        "    def read_restrictions(self):\n",
        "        restrictions = {group.read_restriction for group in self.membership_expanded}\n",
        "        restrictions.add(self.read_restriction)\n",
        "        return {x for x in restrictions if x}\n",
        "\n",
        "    @property\n",
        "    def write_restrictions(self):\n",
        "        restrictions = {group.write_restriction for group in self.membership_expanded}\n",
        "        restrictions.add(self.write_restriction)\n",
        "        return {x for x in restrictions if x}\n",
        "\n",
        "\n",
        "class Individual(Principal):\n",
        "    pass\n",
        "\n",
        "\n",
        "class User(Individual):\n",
        "\n",
        "    instances = {}\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if hasattr(self, 'initialized'):\n",
        "            return\n",
        "        access = Access(scope='user', specifier=self.id)\n",
        "        self.restrict_read(access)\n",
        "        self.restrict_write(access)\n",
        "        self.initialized = ...\n",
        "\n",
        "\n",
        "class Agent(Individual):\n",
        "\n",
        "    instances = {}\n",
        "\n",
        "\n",
        "class Group(Principal):\n",
        "\n",
        "    instances = {}\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if hasattr(self, 'initialized'):\n",
        "            return\n",
        "        self.members = set()\n",
        "        self.initialized = ...\n",
        "\n",
        "    @property\n",
        "    def members_expanded(self):\n",
        "        expanded = set()\n",
        "        stack = list(self.members)\n",
        "        while stack:\n",
        "            member = stack.pop()\n",
        "            expanded.add(member)\n",
        "            if isinstance(member, Group):\n",
        "                stack.extend(member.members)\n",
        "        return expanded\n",
        "\n",
        "    def add_member(self, member):\n",
        "        if member in self.members:\n",
        "            raise ValueError('already a member')\n",
        "        if isinstance(member, Group):\n",
        "            if self.id == member.id:\n",
        "                raise ValueError('cannot be own member')\n",
        "            if self in member.members_expanded:\n",
        "                raise ValueError('cycle detected')\n",
        "        self.members.add(member)\n",
        "        member.membership.add(self)\n",
        "\n",
        "    def remove_member(self, member):\n",
        "        if member not in self.members:\n",
        "            raise ValueError('not a member')\n",
        "        self.members.remove(member)\n",
        "        member.membership.remove(self)\n",
        "\n",
        "    def list_members(self, expanded=False):\n",
        "        \"\"\"List all members of the group.\n",
        "        \n",
        "        Args:\n",
        "            expanded (bool): If True, includes members of nested groups. Default is False.\n",
        "            \n",
        "        Returns:\n",
        "            list: Sorted list of member IDs\n",
        "        \"\"\"\n",
        "        if expanded:\n",
        "            members = self.members_expanded\n",
        "        else:\n",
        "            members = self.members\n",
        "            \n",
        "        return sorted([member.id for member in members])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eH2GHKrDl0t",
      "metadata": {
        "id": "1eH2GHKrDl0t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "e5ac3f31-58b8-4947-88a0-5ce5e50d570b",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "e5ac3f31-58b8-4947-88a0-5ce5e50d570b"
      },
      "outputs": [],
      "source": [
        "def reconcile_restrictions(*restriction_collections):\n",
        "    if len(restriction_collections) < 2:\n",
        "        raise ValueError('at least 2 collections has to be provided')\n",
        "    stack = list(restriction_collections)\n",
        "    first = stack.pop()\n",
        "    while stack:\n",
        "        second = stack.pop()\n",
        "        merged = set()\n",
        "        for first_restriction in first:\n",
        "            for second_restriction in second:\n",
        "                match = first_restriction.match(second_restriction)\n",
        "                if match is not None:\n",
        "                    merged.add(match)\n",
        "        first = merged\n",
        "    # wildcard cannot be present in the output collection\n",
        "    without_wildcard = {x for x in merged if not x.has_wildcard()}\n",
        "    return without_wildcard\n",
        "\n",
        "def get_filter(restrictions):\n",
        "    if not restrictions:\n",
        "        return None\n",
        "    hashes = [x.hash for x in restrictions]\n",
        "    filter_template = 'restrictions:ANY({})'\n",
        "    li = ','.join(f'\"{x}\"' for x in hashes)\n",
        "    the_filter = filter_template.format(li)\n",
        "    return the_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "cf29d543-4ea5-4ca1-8945-e94875394bbb",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "cf29d543-4ea5-4ca1-8945-e94875394bbb"
      },
      "outputs": [],
      "source": [
        "def upload_choices(principal):\n",
        "    return {\n",
        "        ' '.join(x.name for x in restriction.accesses): restriction\n",
        "        for restriction in principal.write_restrictions\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea0f19a-f2b4-4967-b3a3-f54ef7000234",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "bea0f19a-f2b4-4967-b3a3-f54ef7000234"
      },
      "source": [
        "# Interacting with Vertex AI data store's documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "2538a4f4-1710-4728-9279-64775f94ff7f",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "2538a4f4-1710-4728-9279-64775f94ff7f"
      },
      "outputs": [],
      "source": [
        "def get_token():\n",
        "    if os.environ.get('LOCAL'):\n",
        "        output = subprocess.run(['gcloud', 'auth', 'print-access-token'], capture_output=True, text=True)\n",
        "        token = output.stdout.strip()\n",
        "        return token\n",
        "    creds, _ = google.auth.default()\n",
        "    auth_req = google.auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    token = creds.token\n",
        "    return token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "c67c3b77-c946-44d2-a285-2827f1617c21",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "c67c3b77-c946-44d2-a285-2827f1617c21"
      },
      "outputs": [],
      "source": [
        "class Resource:\n",
        "    pass\n",
        "\n",
        "class Row(Resource):\n",
        "    pass\n",
        "\n",
        "@dataclass\n",
        "class Document(Resource):\n",
        "    id: str\n",
        "    restrictions: List[Restriction]\n",
        "    metadata: dict\n",
        "    mime: str\n",
        "    uri: str\n",
        "\n",
        "    @property\n",
        "    def title(self):\n",
        "        return self.uri.rsplit('/', maxsplit=1)[-1].split('.')[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def _headers():\n",
        "        token = get_token()\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer {token}'\n",
        "        }\n",
        "        return headers\n",
        "\n",
        "    @classmethod\n",
        "    def list_all(cls):\n",
        "        response = requests.get(LIST_DOCUMENTS_URL, headers=cls._headers())\n",
        "        return response.json()\n",
        "\n",
        "    def as_resource(self):\n",
        "        metadata = deepcopy(self.metadata)\n",
        "        metadata['restrictions'] = [\n",
        "            x.hash if isinstance(x, Restriction) else x\n",
        "            for x in self.restrictions\n",
        "        ]\n",
        "        resource = {\n",
        "            \"name\": f\"projects/{PROJECT_NUMBER}/locations/{LOCATION}/collections/default_collection/dataStores/{DATA_STORE_ID}/branches/0/documents/{self.id}\",\n",
        "            \"id\": self.id,\n",
        "            \"schemaId\": \"default_schema\",\n",
        "            \"structData\": metadata,\n",
        "            \"parentDocumentId\": self.id,\n",
        "            \"content\": {\n",
        "                \"mimeType\": self.mime,\n",
        "                \"uri\": self.uri\n",
        "            }\n",
        "        }\n",
        "        return resource\n",
        "\n",
        "    def create(self):\n",
        "        response = requests.post(\n",
        "            CREATE_DOCUMENT_URL_TEMPLATE.format(self.id),\n",
        "            headers=self._headers(),\n",
        "            json=self.as_resource()\n",
        "        )\n",
        "        print(response)\n",
        "        return response.json()\n",
        "\n",
        "    @classmethod\n",
        "    def from_id(cls, doc_id):\n",
        "        resource = cls.get(doc_id)\n",
        "        metadata = resource['structData']\n",
        "        restrictions = metadata.pop('restrictions', [])\n",
        "        mime = resource['content']['mimeType']\n",
        "        uri = resource['content']['uri']\n",
        "        return cls(doc_id, restrictions, metadata, mime, uri)\n",
        "\n",
        "    @staticmethod\n",
        "    def get(doc_id):\n",
        "        response = requests.get(\n",
        "            DOCUMENT_URL_TEMPLATE.format(doc_id),\n",
        "            headers=Document._headers()\n",
        "        )\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f'status_code: {response.status_code}')\n",
        "        return response.json()\n",
        "\n",
        "    def wait_till_indexed(self):\n",
        "        sleep_interval = 5\n",
        "        max_waiting_time = sleep_interval * 60\n",
        "        start = time.perf_counter()\n",
        "        while time.perf_counter() - start < max_waiting_time:\n",
        "            print('checking index status...')\n",
        "            response = self.get(self.id)\n",
        "            if 'indexTime' in response:\n",
        "                break\n",
        "            time.sleep(sleep_interval)\n",
        "        return response\n",
        "\n",
        "    def patch(self):\n",
        "        response = requests.patch(\n",
        "            DOCUMENT_URL_TEMPLATE.format(self.id),\n",
        "            headers=self._headers(),\n",
        "            json=self.as_resource()\n",
        "        )\n",
        "        return response.json()\n",
        "\n",
        "    def delete(self):\n",
        "        response = requests.delete(\n",
        "            DOCUMENT_URL_TEMPLATE.format(self.id),\n",
        "            headers=self._headers()\n",
        "        )\n",
        "        return response.json()\n",
        "\n",
        "    def update(self):\n",
        "        # updating could be done with:\n",
        "        # print(self.patch())\n",
        "        # ---\n",
        "        # alternatively with import method (re-importing using in place documents in the API request)\n",
        "        # ---\n",
        "        # at the moment using \"delete and re-create\" workaround\n",
        "        # as patching and importing currently doesn't work correctly due to an internal error:\n",
        "        # \"Path: does not start with gs://\"\n",
        "        print(self.delete())\n",
        "        print(self.create())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6fd46d2-2b0f-4021-8b83-9cc4f763bd7e",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "c6fd46d2-2b0f-4021-8b83-9cc4f763bd7e"
      },
      "source": [
        "# Searching in a data store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "5dfeb31e-0ce6-48e3-a913-2766731f3ffe",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "5dfeb31e-0ce6-48e3-a913-2766731f3ffe"
      },
      "outputs": [],
      "source": [
        "def search(query, search_filter):\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': f'Bearer {get_token()}'\n",
        "    }\n",
        "    payload = {\n",
        "        \"query\": query,\n",
        "        \"pageSize\": 100,\n",
        "        \"queryExpansionSpec\": {\"condition\": \"AUTO\"},\n",
        "        \"spellCorrectionSpec\": {\"mode\": \"AUTO\"},\n",
        "        \"languageCode\": \"en-US\",\n",
        "        \"contentSearchSpec\": {\n",
        "            \"searchResultMode\": \"CHUNKS\",\n",
        "            \"snippetSpec\": {\"returnSnippet\": True}\n",
        "        },\n",
        "        \"filter\": search_filter,\n",
        "        \"relevanceThreshold\": \"MEDIUM\"\n",
        "    }       \n",
        "    response = requests.post(SEARCH_URL, headers=headers, json=payload)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "f9010d49",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_without_filter(query):\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': f'Bearer {get_token()}'\n",
        "    }\n",
        "    payload = {\n",
        "        \"query\": query,\n",
        "        \"pageSize\": 100,\n",
        "        \"queryExpansionSpec\": {\"condition\": \"AUTO\"},\n",
        "        \"spellCorrectionSpec\": {\"mode\": \"AUTO\"},\n",
        "        \"languageCode\": \"en-US\",\n",
        "        \"contentSearchSpec\": {\n",
        "            \"searchResultMode\": \"CHUNKS\",\n",
        "            \"snippetSpec\": {\"returnSnippet\": True}\n",
        "        },\n",
        "        \"relevanceThreshold\": \"MEDIUM\"\n",
        "    }\n",
        "    response = requests.post(SEARCH_URL, headers=headers, json=payload)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32aef5c3-44c7-4d35-8f0d-f6f3c4c6c9d6",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "32aef5c3-44c7-4d35-8f0d-f6f3c4c6c9d6"
      },
      "source": [
        "# Building a LangGraph-based multi-agent application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "a269d1a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_tense(tense: Literal['future', 'past', 'n/a']):\n",
        "    \"\"\"used to indicate if the given phrase is about future, past or none of them (n/a)\"\"\"\n",
        "\n",
        "def tools_or_another_agent(state):\n",
        "    print('redirecting to a different agent, calling the tools or finishing...')\n",
        "    return state.get('next_agent') or tools_condition(state)\n",
        "\n",
        "def last_agent(state):\n",
        "    print('redirecting to the last agent...')\n",
        "    return state['agent_history'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "62cec254-0200-4c95-b2df-ce64449e7828",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "62cec254-0200-4c95-b2df-ce64449e7828"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[List[str], add_messages]\n",
        "    agent_history: Annotated[List[str], operator.add]\n",
        "    next_agent: Optional[str]\n",
        "    retrieved_documents: Annotated[List[list], operator.add]\n",
        "\n",
        "@tool\n",
        "def search_tool(query: str, config: RunnableConfig, state: Annotated[dict, InjectedState]):\n",
        "    \"\"\"this search tool has to be called for any queries on specific companies\"\"\"\n",
        "    current_agent = state['agent_history'][-1]\n",
        "    agent_to_filter = config['configurable']['user_info']['agent_to_filter']\n",
        "    print(f'agent to filter from search_tool = {agent_to_filter}')\n",
        "    search_filter = agent_to_filter[current_agent]\n",
        "    print(f'search_filter from search_tool = {search_filter}')\n",
        "    print(f'query from search_tool = {query}')\n",
        "    resp = search(query, search_filter)\n",
        "    print(f'search_response from search tool = {resp}')\n",
        "    titles = set()\n",
        "    chunks = []\n",
        "    try:\n",
        "        for result in resp['results']:\n",
        "            titles.add(result['chunk']['documentMetadata']['title'])\n",
        "            chunks.append(result['chunk']['content'])\n",
        "    except KeyError:\n",
        "        print(resp)\n",
        "    state['retrieved_documents'].append(sorted(titles))  # workaround for debug purposes as LangGraph doesn't support state modifications with tools atm\n",
        "    return list(dict.fromkeys(chunks))  # dedup as the chunks here come from identical docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "0de7253a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_agent(name, llm, tools):\n",
        "    def agent(state):\n",
        "        print(name)\n",
        "        messages = state['messages']\n",
        "        \n",
        "        # Convert messages to the format Gemini expects\n",
        "        formatted_messages = []\n",
        "        for message in messages:\n",
        "            if hasattr(message, 'content') and hasattr(message, 'type'):\n",
        "                # Handle LangChain message objects\n",
        "                role = 'system' if message.type == 'system' else 'user' if message.type == 'human' else 'assistant'\n",
        "                formatted_messages.append({\n",
        "                    'role': role,\n",
        "                    'content': message.content or \"Empty content\"  # Ensure content is never empty\n",
        "                })\n",
        "            elif isinstance(message, dict):\n",
        "                # Handle dictionary format messages\n",
        "                role = message.get('role', 'user')\n",
        "                content = message.get('content', \"Empty content\")\n",
        "                formatted_messages.append({\n",
        "                    'role': role,\n",
        "                    'content': content\n",
        "                })\n",
        "            else:\n",
        "                # Handle any other type of message\n",
        "                formatted_messages.append({\n",
        "                    'role': 'user',\n",
        "                    'content': str(message)\n",
        "                })\n",
        "        \n",
        "        # Ensure there's always at least one message\n",
        "        if not formatted_messages:\n",
        "            formatted_messages.append({\n",
        "                'role': 'user',\n",
        "                'content': \"Hello\"\n",
        "            })\n",
        "            \n",
        "        # Check if should route to another agent\n",
        "        last_the_same = state['agent_history'] and state['agent_history'][-1] == name\n",
        "        if state.get('next_agent') != name and not last_the_same:\n",
        "            try:\n",
        "                # Use a simpler prompt format for routing decisions\n",
        "                llm_with_tools = llm.bind_tools([detect_tense], tool_choice=True)\n",
        "                system_msg = {\n",
        "                    'role': 'system', \n",
        "                    'content': \"Given this conversation, detect if the next step is to deal with something to do with HVAC docs or Security based docs. If it's not clear, use 'n/a'.\"\n",
        "                }\n",
        "                \n",
        "                # Make sure system message goes first\n",
        "                routing_messages = [system_msg] + [m for m in formatted_messages if m['role'] != 'system']\n",
        "                \n",
        "                response = llm_with_tools.invoke(routing_messages)\n",
        "                tense = response.tool_calls[0]['args']['tense']\n",
        "                if tense != 'n/a':\n",
        "                    return {'agent_history': [name], 'next_agent': tense}\n",
        "            except Exception as e:\n",
        "                # If routing fails, continue with normal processing\n",
        "                print(f\"Routing error: {e}\")\n",
        "        \n",
        "        # Proceed normally\n",
        "        try:\n",
        "            llm_with_tools = llm.bind_tools(tools)\n",
        "            # Ensure at least one non-system message\n",
        "            has_user_message = any(m['role'] == 'user' for m in formatted_messages)\n",
        "            if not has_user_message:\n",
        "                formatted_messages.append({'role': 'user', 'content': 'Hello'})\n",
        "                \n",
        "            response = llm_with_tools.invoke(formatted_messages)\n",
        "            return {'messages': [response], 'agent_history': [name], 'next_agent': None, 'retrieved_documents': []}\n",
        "        except Exception as e:\n",
        "            # Fallback response if model invoke fails\n",
        "            print(f\"Model invocation error: {e}\")\n",
        "            from langchain_core.messages import AIMessage\n",
        "            fallback_msg = AIMessage(content=\"I encountered an error processing your request. Please try again.\")\n",
        "            return {'messages': [fallback_msg], 'agent_history': [name], 'next_agent': None, 'retrieved_documents': []}\n",
        "    \n",
        "    return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "abf71956-6296-4791-988a-9aab7fcb6623",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "abf71956-6296-4791-988a-9aab7fcb6623"
      },
      "outputs": [],
      "source": [
        "def runnable_builder(model, *args, **kwargs):\n",
        "    workflow = StateGraph(State)\n",
        "\n",
        "    toolkit = [search_tool]\n",
        "    tool_node = ToolNode(toolkit)\n",
        "\n",
        "    workflow.add_node('coordinator', make_agent('coordinator', model, toolkit))\n",
        "    workflow.add_node('hvacdoc', make_agent('hvacdoc', model, toolkit))\n",
        "    workflow.add_node('secdoc', make_agent('secdoc', model, toolkit))\n",
        "    workflow.add_node('tools', tool_node)\n",
        "\n",
        "    workflow.add_edge(START, 'coordinator')\n",
        "    workflow.add_conditional_edges('coordinator', tools_or_another_agent)\n",
        "    workflow.add_conditional_edges('hvacdoc', tools_or_another_agent)\n",
        "    workflow.add_conditional_edges('secdoc', tools_or_another_agent)\n",
        "    workflow.add_conditional_edges('tools', last_agent)\n",
        "\n",
        "    graph = workflow.compile()\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d01e440e-ed59-40a2-bbae-480825d9adf6",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "d01e440e-ed59-40a2-bbae-480825d9adf6"
      },
      "source": [
        "# Agent Engine deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "3d222766-28ac-4327-b371-2e059cd044f1",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "3d222766-28ac-4327-b371-2e059cd044f1"
      },
      "outputs": [],
      "source": [
        "vertexai.init(\n",
        "    project=PROJECT_ID, \n",
        "    location=\"us-central1\",\n",
        "    staging_bucket=f'gs://{BUCKET_NAME}' \n",
        ")\n",
        "\n",
        "re_agent = LangchainAgent(\n",
        "    model='gemini-2.0-flash',\n",
        "    runnable_builder=runnable_builder\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "afdb2489-2c92-462b-9d04-171626c2c1cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "afdb2489-2c92-462b-9d04-171626c2c1cd",
        "outputId": "7fc1187d-66d7-4673-bd48-c188ffb04889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified the following requirements: {'google-cloud-aiplatform': '1.87.0', 'cloudpickle': '3.0.0'}\n",
            "The final list of requirements: ['google-cloud-aiplatform[agent_engines,langchain]', 'cloudpickle==3.0.0', 'pydantic==2.11.2', 'requests', 'google-auth', 'langgraph', 'langchain-core']\n",
            "Using bucket data_storage_aiml_hugo\n",
            "Wrote to gs://data_storage_aiml_hugo/agent_engine/agent_engine.pkl\n",
            "Writing to gs://data_storage_aiml_hugo/agent_engine/requirements.txt\n",
            "Creating in-memory tarfile of extra_packages\n",
            "Writing to gs://data_storage_aiml_hugo/agent_engine/dependencies.tar.gz\n",
            "Creating AgentEngine\n",
            "Create AgentEngine backing LRO: projects/1077649599081/locations/us-central1/reasoningEngines/2452922480638033920/operations/849411318443147264\n",
            "View progress and logs at https://console.cloud.google.com/logs/query?project=big-data-379417\n",
            "AgentEngine created. Resource name: projects/1077649599081/locations/us-central1/reasoningEngines/2452922480638033920\n",
            "To use this AgentEngine in another session:\n",
            "agent_engine = vertexai.agent_engines.get('projects/1077649599081/locations/us-central1/reasoningEngines/2452922480638033920')\n"
          ]
        }
      ],
      "source": [
        "remote_agent = agent_engines.create(\n",
        "    re_agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[agent_engines,langchain]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.11.2\",\n",
        "        \"requests\",\n",
        "        \"google-auth\",\n",
        "        \"langgraph\",\n",
        "        \"langchain-core\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d4b49981-ea3d-4ed1-ad0c-d35fec00ded8",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "d4b49981-ea3d-4ed1-ad0c-d35fec00ded8"
      },
      "outputs": [],
      "source": [
        "remote_app = agent_engines.AgentEngine('projects/1077649599081/locations/us-central1/reasoningEngines/4354708160783581184')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3543ba1-7895-4e29-9d9d-0ba3cb7efc73",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "e3543ba1-7895-4e29-9d9d-0ba3cb7efc73"
      },
      "source": [
        "# Helper functions for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "13008a72-72f1-4295-a021-432b92e30ef8",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "13008a72-72f1-4295-a021-432b92e30ef8"
      },
      "outputs": [],
      "source": [
        "def show_environment():\n",
        "    print('environment:')\n",
        "    print('\\t', Document.list_all())\n",
        "    print('\\t', Group.all())\n",
        "    print('\\t', User.all())\n",
        "    print('\\t', Agent.all())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "1bcb9e04-f4b7-48de-9045-ccefe2a8ddf9",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "1bcb9e04-f4b7-48de-9045-ccefe2a8ddf9"
      },
      "outputs": [],
      "source": [
        "def clear_environment():\n",
        "    for doc_id in [x['id'] for x in Document.list_all().get('documents', [])]:\n",
        "        Document.from_id(doc_id).delete()\n",
        "    Group.clear_all()\n",
        "    User.clear_all()\n",
        "    Agent.clear_all()\n",
        "\n",
        "\n",
        "# def clear_environment():\n",
        "#     all_docs_result = Document.list_all()\n",
        "#     doc_list = all_docs_result. # Use .get() for safety\n",
        "#     if doc_list: # Check if the list is not empty\n",
        "#          for doc_id in [x['id'] for x in doc_list]:\n",
        "#              try: # Add try/except for robustness during deletion\n",
        "#                 doc = Document.from_id(doc_id)\n",
        "#                 if doc: # Ensure from_id didn't return None\n",
        "#                     doc.delete()\n",
        "#              except Exception as e:\n",
        "#                  print(f\"Warning: Failed to delete doc {doc_id}: {e}\")\n",
        "#     Group.clear_all()\n",
        "#     User.clear_all()\n",
        "#     Agent.clear_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "06a24d9e-08dd-4b9d-a90d-a9489601b9ee",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "06a24d9e-08dd-4b9d-a90d-a9489601b9ee"
      },
      "outputs": [],
      "source": [
        "def final_response(state):\n",
        "    return state['messages'][-1]['kwargs']['content']\n",
        "\n",
        "def retrieved_documents(state_or_search_results):\n",
        "    if 'retrieved_documents' in state_or_search_results:\n",
        "        return state_or_search_results['retrieved_documents'][-1]\n",
        "    \n",
        "    docs = set()\n",
        "    for result in state_or_search_results.get('results', []):\n",
        "        if 'chunk' in result and 'name' in result['chunk']:\n",
        "            # Extract document ID from the path\n",
        "            path = result['chunk']['name']\n",
        "            parts = path.split('/')\n",
        "            \n",
        "            # Find the document ID in the path after 'documents'\n",
        "            try:\n",
        "                doc_index = parts.index('documents')\n",
        "                if doc_index + 1 < len(parts):\n",
        "                    doc_id = parts[doc_index + 1]\n",
        "                    \n",
        "                    # In a real environment, we might need to look up the actual title\n",
        "                    # For now, just use the document ID as its identifier\n",
        "                    docs.add(doc_id)\n",
        "            except ValueError:\n",
        "                # 'documents' not found in path\n",
        "                continue\n",
        "    \n",
        "    return sorted(docs)\n",
        "# def retrieved_documents(state_or_search_results):\n",
        "#     if 'retrieved_documents' in state_or_search_results:\n",
        "#         return state_or_search_results['retrieved_documents'][-1]\n",
        "#     docs = {x['chunk']['documentMetadata']['title'] for x in state_or_search_results.get('results', [])}\n",
        "#     return sorted(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "7eae9aa5-ed3d-47b2-a8bf-7d09909d3079",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "7eae9aa5-ed3d-47b2-a8bf-7d09909d3079"
      },
      "outputs": [],
      "source": [
        "def confirm_retrievable(query, doc_title):\n",
        "    sleep_interval = 10\n",
        "    max_waiting_time = sleep_interval * 60\n",
        "    start = time.perf_counter()\n",
        "    successful_attempts = 0\n",
        "    want_successful = 5\n",
        "    while time.perf_counter() - start < max_waiting_time:\n",
        "        print('retrieving... ', end='')\n",
        "        search_results = search_without_filter(query)\n",
        "        retrieved = retrieved_documents(search_results)\n",
        "        if doc_title in retrieved:\n",
        "            print('retrieved')\n",
        "            successful_attempts += 1\n",
        "        else:\n",
        "            print('not retrieved')\n",
        "            successful_attempts = 0\n",
        "        if successful_attempts == want_successful:\n",
        "            break\n",
        "        time.sleep(sleep_interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "81e04586-bf9b-4435-82ff-dab8149339bb",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "81e04586-bf9b-4435-82ff-dab8149339bb"
      },
      "outputs": [],
      "source": [
        "def confirm_restrictions(query, doc):\n",
        "    sleep_interval = 10\n",
        "    max_waiting_time = sleep_interval * 60\n",
        "    start = time.perf_counter()\n",
        "    successful_attempts = 0\n",
        "    want_successful = 5\n",
        "    while time.perf_counter() - start < max_waiting_time:\n",
        "        print('checking restrictions... ', end='')\n",
        "        search_results = search_without_filter(query)\n",
        "        retrieved_restrictions = (\n",
        "            x['chunk']['documentMetadata']['structData']['restrictions']\n",
        "            for x in search_results.get('results', [])\n",
        "            if x['chunk']['documentMetadata']['title'] == doc.title\n",
        "        )\n",
        "        retrieved_restrictions = sorted(next(retrieved_restrictions, []))\n",
        "        document_restrictions = sorted(\n",
        "            x.hash if isinstance(x, Restriction) else x\n",
        "            for x in doc.restrictions\n",
        "        )\n",
        "        if retrieved_restrictions == document_restrictions:\n",
        "            print('matching')\n",
        "            successful_attempts += 1\n",
        "        else:\n",
        "            print('not matching')\n",
        "            successful_attempts = 0\n",
        "        if successful_attempts == want_successful:\n",
        "            break\n",
        "        time.sleep(sleep_interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24d8ad97-fdd1-432b-8760-ab0bce1a2d62",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "24d8ad97-fdd1-432b-8760-ab0bce1a2d62"
      },
      "source": [
        "# Test scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b38bd3-5e2b-41c3-b701-e8f412c69298",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "91b38bd3-5e2b-41c3-b701-e8f412c69298"
      },
      "source": [
        "## Scenario 1\n",
        "This scenario demonstrates how different users access disjoint sets of documents based on their permissions.\n",
        "\n",
        "**Setup**\n",
        "\n",
        "- **User U1** is a member of customer organization HVAC_Users\n",
        "- **User U2** is a member of customer organization C2\n",
        "- **User U1** has read access to resources in the HVAC_Documents scope with specifier HVAC_Users\n",
        "- **User U2** has read access to resources belonging to customer organization C2\n",
        "- **Document D1** belongs to HVAC_Documents with specifier HVAC_Users\n",
        "- **Document D2** belongs to customer organization C2\n",
        "- Both documents are relevant for the same question Q1\n",
        "- **Agent A1** has read access to both organizations' resources\n",
        "\n",
        "\n",
        "Step 1: Environment Configuration\n",
        "- The system establishes groups, permissions, and document restrictions\n",
        "- Users are assigned to their respective groups\n",
        "- Documents are created with appropriate access restrictions\n",
        "- The agent is given access to both groups\n",
        "\n",
        "Step 2: Agent Access Verification\n",
        "- A direct search by Agent A1 confirms it can access all documents\n",
        "- This verifies that the agent has proper permissions to both document sets\n",
        "\n",
        "Step 3: User U1 Query\n",
        "- User U1 asks question Q1\n",
        "- The system calculates the intersection of User U1's permissions and Agent A1's permissions\n",
        "- Agent A1 searches using this combined permission filter\n",
        "- Results show that User U1 can access Document D1 (HVAC_Users document)\n",
        "- Document D2 is filtered out as User U1 lacks access to C2 resources\n",
        "\n",
        "Step 4: User U2 Query\n",
        "- User U2 asks the same question Q1\n",
        "- The system calculates the intersection of User U2's permissions and Agent A1's permissions\n",
        "- Agent A1 searches using this combined permission filter\n",
        "- Results show that User U2 can access Document D2 (C2 document)\n",
        "- Document D1 is filtered out as User U2 lacks access to HVAC_Users resources\n",
        "\n",
        "Key Concepts Demonstrated\n",
        "\n",
        "- Users can only access documents from their organization\n",
        "- The same agent can serve multiple users with different access levels\n",
        "- Document visibility is dynamically determined based on user permissions\n",
        "- The system enforces proper information boundaries between organizations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0654b26f",
      "metadata": {},
      "source": [
        "### Scenario 1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "oMvx572whdcy",
      "metadata": {
        "id": "oMvx572whdcy"
      },
      "outputs": [],
      "source": [
        "Group('CustomerA').restrict_read(Access(scope='HVAC_Documents', specifier='HVAC_Users'))\n",
        "Group('CustomerA').add_member(User('U1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "0Ayq_D3bdnXO",
      "metadata": {
        "id": "0Ayq_D3bdnXO"
      },
      "outputs": [],
      "source": [
        "for doc_id in [x['id'] for x in Document.list_all()['documents']]:\n",
        "    Document.from_id(doc_id).delete()\n",
        "Group.clear_all()\n",
        "User.clear_all()\n",
        "Agent.clear_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "96d30fbc-f9b2-449e-a8a5-d9771b903038",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "96d30fbc-f9b2-449e-a8a5-d9771b903038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1.\n",
            "environment:\n",
            "\t {}\n",
            "\t {}\n",
            "\t {}\n",
            "\t {}\n",
            "<Response [200]>\n",
            "{'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1', 'id': 'D1', 'schemaId': 'default_schema', 'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}, 'parentDocumentId': 'D1', 'content': {'mimeType': 'application/pdf', 'uri': 'gs://data_storage_aiml_hugo/adaptation_of_foundation_models_whitepaper_google_cloud.pdf'}}\n",
            "checking index status...\n",
            "checking index status...\n",
            "checking index status...\n",
            "checking index status...\n",
            "{'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1', 'id': 'D1', 'schemaId': 'default_schema', 'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}, 'parentDocumentId': 'D1', 'content': {'mimeType': 'application/pdf', 'uri': 'gs://data_storage_aiml_hugo/adaptation_of_foundation_models_whitepaper_google_cloud.pdf'}, 'indexTime': '2025-04-08T18:39:16.284879Z', 'indexStatus': {'pendingMessage': 'Document ingestion and parsing have finished. Document indexing is working in progress.'}}\n",
            "<Response [200]>\n",
            "{'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2', 'id': 'D2', 'schemaId': 'default_schema', 'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}, 'parentDocumentId': 'D2', 'content': {'mimeType': 'application/pdf', 'uri': 'gs://data_storage_aiml_hugo/ai_adoption_framework_whitepaper.pdf'}}\n",
            "checking index status...\n",
            "checking index status...\n",
            "checking index status...\n",
            "checking index status...\n",
            "{'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2', 'id': 'D2', 'schemaId': 'default_schema', 'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}, 'parentDocumentId': 'D2', 'content': {'mimeType': 'application/pdf', 'uri': 'gs://data_storage_aiml_hugo/ai_adoption_framework_whitepaper.pdf'}, 'indexTime': '2025-04-08T18:39:35.383903Z', 'indexStatus': {'pendingMessage': 'Document ingestion and parsing have finished. Document indexing is working in progress.'}}\n",
            "['U2', 'coordinator']\n",
            "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'what is an embedding?'}]\n",
            "environment:\n",
            "\t {'documents': [{'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1', 'id': 'D1', 'schemaId': 'default_schema', 'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}, 'parentDocumentId': 'D1', 'content': {'mimeType': 'application/pdf', 'uri': 'gs://data_storage_aiml_hugo/adaptation_of_foundation_models_whitepaper_google_cloud.pdf'}, 'indexStatus': {'pendingMessage': 'Document ingestion and parsing have finished. Document indexing is working in progress.'}}, {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2', 'id': 'D2', 'schemaId': 'default_schema', 'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}, 'parentDocumentId': 'D2', 'content': {'mimeType': 'application/pdf', 'uri': 'gs://data_storage_aiml_hugo/ai_adoption_framework_whitepaper.pdf'}, 'indexStatus': {'pendingMessage': 'Document ingestion and parsing have finished. Document indexing is working in progress.'}}]}\n",
            "\t {'HVAC_Users': Group('HVAC_Users'), 'C2': Group('C2')}\n",
            "\t {'U1': User('U1'), 'U2': User('U2')}\n",
            "\t {'coordinator': Agent('coordinator')}\n"
          ]
        }
      ],
      "source": [
        "print('Step 1.')\n",
        "\"\"\"Users with different permissions accessing disjoint sets of documents\"\"\"\n",
        "clear_environment()\n",
        "show_environment()\n",
        "documents = [\n",
        "    Document(\n",
        "        id='D1',\n",
        "        restrictions=[Restriction(Access(scope='HVAC_Documents', specifier='HVAC_Users'))],\n",
        "        metadata={},\n",
        "        mime='application/pdf',\n",
        "        uri='gs://data_storage_aiml_hugo/adaptation_of_foundation_models_whitepaper_google_cloud.pdf'\n",
        "    ),\n",
        "    Document(\n",
        "        id='D2',\n",
        "        restrictions=[Restriction(Access(scope='customer', specifier='C2'))],\n",
        "        metadata={},\n",
        "        mime='application/pdf',\n",
        "        uri='gs://data_storage_aiml_hugo/ai_adoption_framework_whitepaper.pdf'\n",
        "    )\n",
        "]\n",
        "for doc in documents:\n",
        "    print(doc.create())\n",
        "    print(doc.wait_till_indexed())\n",
        "Group('HVAC_Users').restrict_read(Access(scope='HVAC_Documents', specifier='HVAC_Users'))\n",
        "Group('HVAC_Users').add_member(User('U1'))\n",
        "Group('C2').restrict_read(Access(scope='customer', specifier='C2'))\n",
        "Group('C2').add_member(User('U2'))\n",
        "agent_A1 = 'coordinator'\n",
        "Group('HVAC_Users').add_member(Agent(agent_A1))\n",
        "Group('C2').add_member(Agent(agent_A1))\n",
        "print(Group('C2').list_members())\n",
        "question_Q1 = 'what is an embedding?'\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {'role': 'user', 'content': question_Q1}\n",
        "]\n",
        "print(messages)\n",
        "show_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b639fd9",
      "metadata": {},
      "source": [
        "### Scenario 1 Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "27ac3e47",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step #2\n",
            "search_filter = restrictions:ANY(\"257f33442c74c67017fb982b6582c684\",\"f9b857ba14374d8c5f7fe05067f6d210\")\n",
            "search_results = {'results': [{'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/8', 'id': '8', 'content': '6\\n\\nEach of the themes draws its character from the two areas it bridges:\\n\\nLearn concerns the quality and scale of learning\\nprograms to upskill your staff, hire external talent, and\\naugment your data science and ML engineering staff\\nwith experienced partners. What data and ML skill sets\\nare required in the organization? What data science and\\nengineering roles should be hired? To what extent do\\nlearning plans reflect business needs? What is the\\nnature of the partnership with AI third parties?\\n\\nLead concerns the extent to which your data scientists\\nare supported by a mandate from leadership to apply\\nML to business use cases, and the degree to which the\\ndata scientists are cross-functional, collaborative, and\\nself-motivated. How are the teams structured? Do they\\nhave executive sponsorship and empowerment? How\\nare AI projects budgeted, governed, assessed?\\n\\nAccess concerns the extent to which your organization\\nrecognizes data management as a key element to\\nenable AI and the degree to which data scientists can\\nshare, discover, and reuse data and other ML artifacts.\\nHow is the dataset created, curated, and annotated?\\nWho owns the dataset? Is it discoverable and reusable?\\nCan you share, reuse, and expand trained models,\\nnotebooks, and other ML components and solutions?', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/7', 'id': '7', 'content': '5\\n\\nLeveraging the power of AI\\n\\nHow do you structure your teams for success? How can you create, discover, share, and\\nmanage data assets? How can you leverage native cloud technologies to scale AI? How do\\nyou streamline the process of updating and monitoring your ML models in production?\\n\\nWe have a solution for that.\\n\\nThe AI maturity themes\\n\\nGoogle Cloud’s AI Adoption Framework is anchored in the familiar rubric of people, process,\\ntechnology, and data. The interplay between these four key areas gives rise to six themes:\\nLearn, Lead, Access, Scale, Automate, and Secure. These themes are foundational to the AI\\nAdoption Framework.\\n\\nFigure 1: The AI maturity themes', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/4', 'id': '4', 'content': 'With Google Cloud’s AI Adoption Framework, you’ll be able to create\\nand evolve your own transformative AI capability. You’ll have a map\\nfor assessing where you are in the journey and where, at the end of it,\\nyou’d like to be. You’ll have a structure for building scalable AI\\ncapabilities to create better insights from big data with powerful\\nalgorithms across the entire business.\\nWith Google Cloud as your guide, the path to AI is considerably\\nsmoother.\\n\\n2', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/3', 'id': '3', 'content': '1\\n\\nCreating value through AI, every step\\nof the way\\n\\nCompanies everywhere are seeking to leverage the power of AI. And\\nrightly so. The smart applications of AI enable organizations to\\nimprove, to scale, and to accelerate the decision-making process\\nacross most business functions, so as to work both more efficiently\\nand more effectively. It can also open up new avenues and new\\nrevenue streams, providing the organization with an additional\\ncompetitive edge.\\n\\nIn short, many believe (as we do) that the enterprises that invest in\\nbuilding industry-specific AI solutions today are positioning\\nthemselves to be the global economic leaders of tomorrow.\\n\\nBut the path to building an effective AI capability is not an easy one.\\nThere are many challenges to overcome. Challenges with the\\ntechnology to develop platforms and solutions. With the people who\\nwill implement and manage that technology. With the data that fuels\\nthe technology. And with the processes that govern the whole of it.\\nHow do you harness the power inherent in AI, while avoiding any\\npotential missteps?\\n\\nThat’s where Google Cloud comes in. Our framework for AI adoption\\nprovides a guide to technology leaders who want to build an effective\\nAI capability, one that enables them to leverage the power of AI to\\nenhance and streamline their business, smoothly and smartly. The\\nframework is informed by Google’s own evolution, innovation, and\\nleadership in AI, including experience deploying AI in production\\nthrough products such as Gmail and Google Photos. It is also inspired\\nby many years of experience helping cloud customers1 — from\\nstartups to enterprises, in various industries — to solve complex\\nchallenges.2\\n\\n1 Google Cloud Customer Voices Digital Book 2019.\\n2 Google Cloud named a leader in The Forrester New Wave™: Computer Vision\\nPlatforms, Q4 2019.\\n\\nRecent advances in\\ntechnology are\\nmaking AI more\\nversatile — and all\\nbut indispensable', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/11', 'id': '11', 'content': '9\\n\\nThe AI Maturity Scale\\nWhen you evaluate the AI maturity themes in light of the three phases, the result is the AI\\nMaturity Scale.\\n\\nSelf-motivated, isolated\\nlearning using online resources\\nThird parties cover the skills\\ngap in the organization\\nNo hiring for ML skills\\n\\nAI adoption driven by individual\\ncontributors\\n“Heroic” project manager with\\nteam budget\\nAI/ML link to business goals\\nnot always clear\\n\\nHiring data science and ML skills\\nOrganizing structured and\\ncontinuous training programs\\nStrategic partner selected to\\nprovide consulting and\\nspecialized knowledge\\n\\nCreating a centralized cross\\nfunctional advanced analytics\\nteam to establish common ML\\npatterns and practices\\nSenior executive sponsorship\\nand dedicated budget by C-level\\nfor innovative projects\\nAligning AI efforts with business\\nobjectives and priorities\\n\\nLearning by embedding data\\nscientists to the business\\nfunction teams\\nHiring data science and ML\\ntalent for innovation with\\nindustry expertise\\nPartnering to innovate, co-create,\\nand augment technical resources\\n\\nEndorsement and dedicated\\nbudget within each line of\\nbusiness\\nFunction-specific data science\\nteams with domain expertise, in\\naddition to the centralized\\nadvanced analytics team\\nInnovation and research teams\\n\\nNo asset sharing\\nIsolated data islands\\nBuilding a data lake\\n\\nManaging an enterprise data\\nwarehouse\\nDefining and sharing a unified\\ndata model\\nCentralized data and ML asset\\nmanagement\\n\\nDiscovering, sharing, and reusing\\ndatasets and AI assets\\nStandardized ML feature stores\\nand datasets\\nEncouraging contributions from\\nacross the organization', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/27', 'id': '27', 'content': 'Access\\n\\nAn organization’s maturity in the Access theme reflects that organization’s ability both to\\naccelerate an ML project and to improve the adoption of the AI capabilities across different\\nteams and functions.\\n\\nTactical maturity\\nEach team usually manages its own data island and ML assets,101 with no sharing among\\nother teams. Even within the same team, it might not be possible to reuse assets, as the\\ntechnology and processes have not been standardized yet.\\nIn a bid to update your data management and access strategy, you are looking into building a\\nunified data lake, containing the raw (structured and unstructured) data feeds, for example,\\nusing Cloud Storage. This data lake can then form the foundation for collecting, curating, and\\nsharing data across the organization, that is, for building managed information stores.\\nStrategic maturity\\nYou recognize data as a vital enterprise asset that fuels AI, and so you’ve improved both the\\nutilization and the management of data, working to keep it safe and useful. You likely have\\ninvested in an EDW, which provides a unified data model, with an integrated, consistent\\ninformation store for various business functions. You have also made data quality\\nmanagement a business priority to ensure the right foundation for analytics and AI. The EDW,\\nwhich can be implemented using BigQuery, empowers data analysts in their reporting, data\\nscientists in their exploratory data analysis and ML experimentation tasks, and other\\nbusiness intelligence (BI) activities.\\n\\n10 ML assets refers to notebooks, trained models, reusable components, and code snippets. From the strategic\\nphase onwards, solution templates are another asset used to create consistency.\\n\\nBridging People and Data, “Access” concerns the extent to which your\\norganization recognizes data management as a key element to enable AI and\\nthe degree to which data scientists can share, discover, and reuse data and\\nother ML assets.\\n\\n25', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0.1]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/1', 'id': '1', 'content': 'Adaptation of Large Foundation Models\\n\\nAbout This Whitepaper\\n\\nTuning a large model involves adjusting and adapting a pre-trained model to perform specific tasks\\nwith the user provided training dataset. This white paper is a technical reference aimed at outlining\\nGoogle’s approach on adapter tuning. Developers who are responsible for deploying machine\\nlearning models, product managers, business leaders, and security engineers will find parts of this\\nwhite paper relevant, particularly around how data is handled in various stages and the current\\nlimitations. Implementation details are current as of July 11th, 2023 and are subject to change.\\n\\nOverview\\n\\nBackground on Foundation Models\\nFoundation models, also known as large-scale pre-trained models, have become a transformative\\nforce in the field of artificial intelligence (AI) and machine learning (ML). These models are\\ntypically trained on vast amounts of diverse data, allowing them to learn general patterns and\\nrepresentations that can be applied across various domains and tasks.\\nThe power of foundation models lies in their ability to learn high-quality, transferable features from\\nthe data they are trained on during the pre-training phase. This pre-training phase enables the\\nmodels to capture relationships and develop an understanding of the data, which can then be\\nfine-tuned to specific tasks. As a result, foundation models have proven to be highly effective in\\nnumerous applications, such as natural language processing, computer vision, and reinforcement\\nlearning.\\nThe widespread success of foundation models can be attributed to several factors, including:\\n● Large-scale data: Foundation models are trained on large datasets, which provide a source\\nof diverse information for learning general patterns and structures. This data exposure\\nallows the models to develop a nuanced understanding of a wide variety of domains and\\ntasks.\\n\\n● Advanced architecture: The architecture of foundation models, such as the Transformer,\\nenables them to capture complex relationships.\\n● Hardware advancements: The performance of foundation models tends to improve with\\nincreased size and computational resources. Researchers and organizations have been\\n\\n1', 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/5', 'id': '5', 'content': 'Security considerations\\nThe adapter layer is stored on Cloud Storage with the option of using CMEK\\n\\n,\\n\\nAccess Transparency\\n\\n3\\n\\nwith audit logging, and multi-party authentication for many of the administrative processes.\\nYou can use VPC Service Controls (VPC-SC) to create a service perimeter that protects the\\nendpoint. The endpoint can be accessed only from within the VPC service perimeter and access\\ncan be controlled through IAM permissions.\\n\\n3\\n\\nCMEK and Access Transparency will be available during General Availability of this feature.\\n\\n5', 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/3', 'id': '3', 'content': \"Design of Adapter Tuning on Vertex AI\\n\\nOur Parameter Efficient Fine Tuning jobs run on Cloud TPUs/GPUs through Vertex AI’s training\\nservice. Vertex AI creates a separate tenant project for each customer project and runs the training\\nworkloads on Compute Engine VMs on the tenant project.\\nDuring the Parameter Efficient Fine Tuning process, the associated VMs in the tenant project load\\nthe frozen model weights and the training dataset that consists of hundreds of prompt/response\\nexamples. The adapter weights are trained by using gradient-based optimization methods, and\\nthen checkpointed to a per-customer bucket in the tenant project during the training. Once the\\ntraining is done, the final adapter weights are stored in the same per-customer bucket for\\ndeployment.\\n\\nSecurity considerations\\nThroughout the training process, user input data remains confined to the user's storage bucket in\\nthe customer project, ensuring no external data transfer. Data can be encrypted by using\\n\\n3\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/2', 'id': '2', 'content': \"able to push the boundaries of these models by scaling them up, leading to even more\\npowerful and capable systems. This scalability enables foundation models to tackle\\nincreasingly complex tasks and achieve state-of-the-art performance across various\\ndomains. New generations of accelerators (e.g. TPUs, GPUs) provide better computation\\nefficiency, lower energy consumption, higher memory size and bandwidth per chip, and\\nbetter connectivity that allows the models to scale up to the current sizes.\\n\\nAdapter Tuning\\nParameter Efficient Fine Tuning (PEFT) is a class of fine-tuning methods that help foundational\\nmodels adapt to specialized tasks by incorporating domain-specific or organization-internal data.\\nThrough the addition of a minimal number of parameters (adapters), these techniques enable\\nefficient model modification and alignment with the target domain.\\nThese techniques enable tuning of the model to specific tasks without having to rebuild the entire\\nfoundation model. Shared deployment of a foundation model can be quickly augmented with\\nadapter weights that are specific to a particular task or domain at runtime. This allows multiple\\nusers of the same foundation model to privately augment and manage the same foundation model\\nwith training that is unique to their needs or deployment requirements.\\n\\nSecurity and Privacy Principles\\n\\nIn this section, we go through the high-level security and privacy principles that guide our design\\nprocess for tuning and serving Foundation Models with respect to Google Cloud and our\\ncustomers' data.\\nCustomer data (e.g. prompts, input training data, etc.) will not be logged or used for improving the\\nGoogle foundation models without the customer’s permission.\\nInput data such as prompts are customer data and stored securely at every step along the way -\\nencrypted at rest and in transit. Adapter models are also stored securely, and Customers will have\\nsole access to use any adapter models.\\nCustomers will be able to control the encryption of stored adapters by using customer-managed\\nencryption keys (CMEK), and delete adapters at any time.1\\n\\n1CMEK will be available at GA\\n\\n2\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/4', 'id': '4', 'content': \"Customer-Managed Encryption Keys (CMEK) on Cloud Storage and accessed via the user-provided\\n\\n2\\n\\nservice account for enhanced security and inside the VPC security perimeter of the customer.\\nTemporary checkpoints generated in the tenant's project bucket are subject to automatic deletion\\nwithin a 30-day period, minimizing the risk of unauthorized access or unintended data retention.\\nAccess to the tenant project by anyone in Google other than the customer, requires a valid business\\njustification, such as a request for Google to assist in debugging an issue or a system outage. Any\\nrequest is logged through Access Transparency logs to maintain a comprehensive record of all\\ninteractions and safeguard against potential security breaches.\\n\\nDesign of Adapter serving on Vertex AI\\n\\nOnce the adapter layers are trained, the weights are uploaded as a model to a bucket in the Vertex\\nPrediction per-customer tenant project and is deployed to an Endpoint in the customer’s project.\\nWhen serving a request on an endpoint, the adapter weights are loaded from the bucket, cached to\\nminimize latency and sent to the foundation model along with the original request (e.g. user's\\nprompt) for inference. The latency overhead of transferring the adapter layers is typically negligible\\ncompared to the inference of a large model.\\nFoundation model service has the frozen model weights loaded during the start up. It receives the\\nadapter weights for the duration of an inference, runs through the request and returns the results,\\nwithout modifying the model or storing the request.\\n\\n2CMEK, User Provided Service accounts and VPC Service controls are currently not supported.\\n\\n4\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/22', 'id': '22', 'content': \"20\\n\\nLearn\\n\\nAn organization’s maturity in the Learn theme reflects that organization’s ability both to keep\\nup with the latest advances in ML and to evolve AI capabilities toward solving ever more\\ncomplex business problems.\\n\\nTactical maturity\\n\\nLearning about data science and ML is self-motivated by a few members of your IT and data\\nteams, using publicy available online resources. While this approach is useful for developing\\ntechnical skills around AI and ML in general, the training courses don’t follow a planned\\nlearning path that is aligned with your organization's current and future needs. Meanwhile, a\\nfew members may be working on ML prototypes, but when faced with a compelling use case\\nwhere ML is needed, your organization tends to turn to third-party consultants, rather than\\nhiring to fill the skill set gaps in-house.\\n\\nStrategic maturity\\n\\nYou’re starting to build greater AI capability in-house by hiring the required data science and\\nML engineering skills. As there are plenty of job titles and descriptions in the field of ML,91\\nyour focus is on the skills and seniority levels that the organization needs.\\n\\n9 For a breakdown of the top 10 role profiles needed in a data science team, see this article from Google’s Head of\\nDecision Intelligence.\\n\\nThe AI Maturity Scale\\n\\nWhen you evaluate the six AI maturity themes in terms of the three AI maturity phases —\\nwhere each phase is descriptive of how a given organization is currently functioning in that\\ntheme — you get the AI Maturity Scale.\\nIn each of the themes, you can see what happens when an organization moves from\\nexperimenting with ML tools and technologies, to working with them more strategically, to\\nbuilding a transformational AI capability.\\n\\nBridging People and Technology, “Learn” concerns the quality and scale of\\nlearning programs to upskill your staff, hire outside talent, and augment your\\ndata science and ML engineering staff with experienced partners.\", 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/13', 'id': '13', 'content': '11\\n\\nIn each of the themes, you can see what happens when you move from adopting AI\\napproaches ad hoc, to working with them more and more comprehensively across the\\norganization — which means deeper and more consistent training for your people, which in\\nturn means streamlined and updated processes, which in turn drives collaboration and, in\\ntime, innovation. The organization transforms.\\nWhen AI has been integrated into all parts of your organization, then you are fully harnessing\\nthe power it offers to transform your position in the industry. But at every step along the way,\\nadding in effective AI capabilities brings benefits.\\n\\nPutting it all together\\nAnd that’s the essence of the framework that Google Cloud uses to guide customers\\nsuccessfully through the process of adopting AI in their organization.\\nWith the framework, you can assess your organization’s AI maturity and determine what\\nyou’ll need to bridge the gap to where you’d like to be. While we touch on the Google Cloud\\nproducts, you can use this information however you would like: the framework is technology\\nagnostic. We’re here to offer further guidance, if that alignment dovetails with your vision.\\nWe’ve worked hard to make AI accessible to all, not only ML researchers and engineers, but\\nto a vast array of customers across industries as well. And our ongoing work in tooling,\\nframeworks, datasets, and models is well documented in the open source community. AI and\\nML are central to who we are.\\n\\nWhether or not we accompany you on the journey, however, our framework can help you find\\nyour way, from your initial changes all the way to becoming fully AI-powered.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/23', 'id': '23', 'content': \"21\\n\\nTo ensure that your team is well qualified to use the necessary tools and technology, you’re\\ndeveloping clear learning paths with certifications and aligning those paths with your\\norganization’s priority use cases. You’ve recognized that the field of AI is constantly changing,\\nand you keep up with those changes by following relevant events, conferences, and experts\\nin the field. You strive to do this systematically, providing employees with training\\nopportunities and requirements to fulfill.\\nIn addition, you choose a strategic partner for advisory, consultation, and program\\nmanagement, someone who also provides specialized knowledge in specific AI and ML use\\ncases (for example, chatbots and conversational apps). You use partnerships to help\\naccelerate your AI adoption by taking advantage of best practices and other organizational\\nlessons that experienced AI and ML subject matter experts bring to the table.\\nTransformational maturity\\nYou aim to head-hunt well-known AI and ML talent to lead increasing research and innovation\\nefforts — while also gaining a greater reputation for effective talent development. Both\\nindustry expertise (like finance, healthcare, telco) and domain expertise (churn prediction,\\ncredit risk assessment, medical diagnostics) become increasingly important for hiring great\\ncandidates. This talent and experience is essential for solving groundbreaking domain\\nspecific problems with AI.\\nFor learning and development, you encourage peer-to-peer and community learning, creating\\nan internal knowledge base, wikis, and tailored courses and learning tracks. You also\\norganize internal conferences to showcase AI applications and promote knowledge sharing.\\nEnabling rotations across business functions ensures not only tighter collaboration between\\nmembers of different teams, but also an exchange of experience and learning on the job. This\\nmechanism for diffusing and developing talent across your organization can become part of\\nyour employee value proposition, helping to attract the best talent. This can, in turn, help start\\na virtuous talent cycle.\\nAt this maturity phase, a partnership can evolve to become a co-creation relationship, where\\nboth you and your partners' ML researchers and engineers work together to break new\\nground and solve cutting-edge problems in the field. Such research also becomes part of\\nyour value proposition for employees, and a competitive advantage in the market.\", 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/25', 'id': '25', 'content': '23\\n\\nStrategic maturity\\n\\nYour senior executives support AI capabilities and projects to deliver value to several\\nbusiness functions, an approach that amounts to a competitive advantage. You’ve begun to\\nestablish a mechanism for standardizing practices and guidelines and sharing accumulated\\nknowledge. To accomplish this, you may have put into place a centralized advanced analytics\\nteam, with a dedicated budget established by a set of C-level executives with a technology\\nagenda.\\n\\nThe advanced analytics team works on delivering prioritized projects or short-term\\nconsulting to other teams. In addition, they proactively evangelize and advocate their AI\\ncapabilities to other lines of business, clarifying how AI can address the various use cases.\\nWhat members of this team all share is a forward-thinking and self-motivated interest in\\nusing AI to deliver business value. Equally important, they share an understanding of the\\nimportance of scaling this impact by growing and embedding data science capabilities in\\nfunctional teams across the business.\\n\\nTransformational maturity\\n\\nEach functional team has its own data scientists with the right domain-specific expertise and\\nsupport from a technical project manager with the right organizational influence. This\\nensures that models can be successfully deployed and scaled in a timely manner. This team\\nis supported by a centralized advanced analytics team, which provides standards and best\\npractices, as well as the tooling and platform for implementing ML projects. Having both a\\ncentralized team and embedded data scientists in the business functions ensures technical\\nstandards and business alignment, respectively.\\n\\nWith standards firmly in place, and the structure to support further investigation, research\\nand innovation activities flourish. This work is sponsored by senior executives both to solve\\ncutting-edge business problems and to establish the organization’s name as a market leader.\\nIn addition, to stimulate innovation, you facilitate competitions that reward employees for\\nsharing ideas on how ML can solve key problems.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/30', 'id': '30', 'content': '28\\n\\nYour data scientists build custom ML models tailored to your data and business needs, using\\nadvanced ML tools and frameworks such as TensorFlow Enterprise. Long-running ML\\ntraining jobs are performed using cloud serverless platforms for distributed training and\\nautomatic hyperparameter tuning — with no dedicated infrastructure — for example, with AI\\nPlatform Training.\\nUsing cloud-native technologies helps your data scientists and engineers focus on data\\nprocessing and ML modeling activities, enabling them to run numerous jobs at scale on large\\ndatasets with no infrastructure operations overhead.\\nTransformational maturity\\nYou’ve invested in building online complex event processing and stream analytics pipelines,\\npowered by ML, to achieve (near) real-time operation optimization and decision-making. And\\nyou’re engaged in architecting an integrated ML platform, for experimentation and\\nproductionalization, to serve all the data scientists and ML engineers in different teams. This\\napproach provides access to standardized tools, services, compute, processes, and best\\npractices, both from internal and external sources. The goal is to give data science teams a\\nsolid starting point when approaching new ML projects and to provide them with the support\\nto carry them through to completion of those projects.\\nYou use ML accelerators (like GPUs or Cloud TPUs111 ) at scale to train complex ML models,\\nusing a large amount of data in a short time. In addition, your data engineering team creates\\nmetadata-driven data processing templates to configure and deploy new workflows without\\ncoding. End-to-end data and ML pipelines are orchestrated and automated with the required\\ninstrumentation.\\n\\n11 A TPU is a Tensor Processing Unit, a chip specifically designed to be faster and more power-efficient than GPUs\\nfor certain machine learning tasks.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/28', 'id': '28', 'content': '26\\n\\nIn addition, your organization is starting now to put together a centralized repository for ML\\nassets and to develop tools for annotating and categorizing data, for example, using Data\\nCatalog. Such assets are created and curated by a centralized advanced analytics team,\\nwhich creates and provides access to such assets on demand. Although each team\\nmaintains their own assets, there is typically limited discoverability and shareability outside\\nthis team.\\n\\nTo facilitate AI adoption, you’re looking now at ways of better handling sensitive data and of\\nbetter protecting data overall.\\nTransformational maturity\\nYou are actively working to develop the technology and processes to allow data scientists to\\ncreate, share, discover, and reuse data and ML assets across different teams and functions,\\nwhich accelerates the launch of new ML-based services. And your teams are using\\nspecialized data storage tools (such as Cloud SQL, Cloud Spanner, Cloud Bigtable, Firestore,\\nand Memorystore) based on the requirements for data volume, structure, and consistency\\nand on read/write workloads.\\n\\nTo help standardize the definition, storage, and access of features for training and serving ML\\nmodels, you have a feature store as a unified repository, which provides functionality for\\nregistering new features to be discovered and used by data scientists. This approach\\nprevents inconsistency between the data used in training the model and the data\\nsubsequently used in serving the model. Data scientists from across the organization are\\nencouraged to use and contribute to the feature store.\\nYour AI efforts are typically driven and maintained by a centralized team that acts as a COE,\\nwith the goal of driving consistency by defining the standards, the procedures, and the\\ntemplates to create different assets, rather than actively creating them.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/34', 'id': '34', 'content': '32\\n\\nAutomate\\n\\nAn organization’s maturity in the Automate theme reflects the ability of its AI systems to adapt\\nto changes in data and the environment, which provides the means for making timely data\\ndriven decisions.\\n\\nTactical maturity\\n\\nThe technologies adopted across your organization are easy to use, such as data wrangling\\ntools, sheet-based data visualization, and ready-to-use AI services. The process of using\\nsuch technologies to build AI systems is usually manual in every step — from data analysis\\nand preparation, to model training and validation. The process is also driven by experimental\\ncode that is written and executed interactively (for example, using Jupyter Notebooks) by\\ndata scientists until a workable model is produced.\\n\\nData analytics and ML are ad hoc, with no automation, meaning that insights are not\\ndelivered regularly in a timely manner. As is appropriate when directed by this manual, data\\nscientist–driven process, your models are rarely changed or retrained. However, such a\\nmanual process prevents you from working with a large number of models or with frequent\\nupdates to these models.\\n\\nStrategic maturity\\nYou automate data processing and analytics pipelines (for example, using Cloud Composer)\\neither on a recurrent schedule (for example, using Cloud Scheduler) or, to tackle critical\\nevents, on a trigger (like data anomaly detection). You’ve started to standardize processes\\nand to consolidate technology to govern the flow of data from one step of the data lifecycle\\nto the next, that is, from data ingestion and transformation to data analysis and reporting.\\nThis strategy has led to increased agility and decreased cycle times, while reducing data\\ndefects, giving developers and business users greater confidence in the insights of analytics\\nfor timely decisions.\\n\\nBridging Technology and Process, “Automate” concerns the extent to which\\nyou are able to deploy, execute, and operate technology for data processing\\nand ML pipelines in production efficiently, frequently, and reliably.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/10', 'id': '10', 'content': '8\\n\\nThe AI maturity phases\\nYour readiness for success in adopting AI in your business is determined by your current\\nbusiness practices in each of these six themes. For each theme, those practices will fall into\\none of the following phases:\\n\\nTactical: Simple use cases for AI are in place, but they are typically\\nshort-term and narrow. There may be no coherent plan with a strategy\\nfor building out to the future.\\nThe focus is on easy adoption, minimal disruption, and quick wins.\\n\\nStrategic: AI is now used to deliver sustainable business value for the\\norganization, with several ML systems deployed and maintained in\\nproduction, leveraging both ready-to-use and custom models.\\n\\nA broader vision governs AI adoption. ML is no longer seen as the\\ndomain of a special few. Perception starts to move beyond the hype,\\nbecoming a pivotal accelerator for the business.\\n\\nTransformational: AI plays a key role in the organization: stimulating\\ninnovation, supporting agility, and helping to cultivate a culture where\\nexperimentation and learning is continuous and encouraged.\\n\\nML expertise has diffused across lines of business. There is a\\nmechanism in place for scaling and promoting ML capabilities across\\nthe organization.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/26', 'id': '26', 'content': '24\\n\\nYour C-level executives act passionately, continuously demonstrating active sponsorship for\\nAI projects and encouraging contributions from across the organization. The leadership\\nfunctions effectively for an AI-driven environment, fostering a culture of blamelessness and\\nopen communication channels, where sharing failures openly is encouraged and mistakes\\nare treated as opportunities for improvement. Experimentation drives innovation and market\\nsuccess. Teams are free to try out many different ideas with the goal of failing faster, failing\\nbetter, and learning from the experience to improve and innovate.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/24', 'id': '24', 'content': '22\\n\\nLead\\n\\nAn organization’s maturity in the Lead theme reflects the effectiveness with which that\\norganization will adopt AI in line with business priorities.\\nTactical maturity\\nAI adoption is driven across your organization by individual contributors or a manager within\\none project team, with little or no executive sponsorship. There is typically limited\\ncollaboration between data scientists in different teams, and there is often an unclear line of\\nsight between ML initiatives and the organization’s strategic goals. You’re identifying the right\\nuse cases for applying ML and also pursuing executive sponsorship. The focus is to explore\\nthe power and prove the value of ML to stakeholders, while the funding is part of the project\\nteam’s budget.\\n\\nBridging People and Process, “Lead” concerns the extent to which your data\\nscientists are supported by a mandate from leadership to apply ML to\\nbusiness use cases, and the degree to which the data scientists are cross\\nfunctional, collaborative, and self-motivated.\\n\\nThe first successful use case\\n\\nThe first step to realizing value from AI is to identify the right business problem and a sponsor\\ncommitted to using AI to solve that problem. To ensure alignment, start with your organization’s\\nbusiness strategy and key priorities. Identify the right business priority use cases to address\\nwith AI, and find the senior executive to own it. Work with their team to get their buy-in and\\nsponsorship. AI projects are more likely to be successful when they have a senior executive\\nsponsor who will champion them with other leaders in your organization.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/31', 'id': '31', 'content': '29\\n\\nSecure\\n\\nAn organization’s maturity in the Secure theme reflects that organization’s ability to ensure that\\ntheir data is appropriately protected, catalogued, encrypted, and guarded from exfiltration, in\\naccordance with ethical AI principles and practices.\\n\\nTactical maturity\\nYour Cloud Identity and Access Management (IAM) policies predominantly rely on the\\nconvenience of project-level primitive roles (Owner, Editor, Viewer) rather than following the\\nprinciple of least privilege. Default permissions allow for any user to create projects and\\nbilling accounts. Cloud IAM permissions are not continuously monitored, and the admin\\nactivity and data access logs are not systematically audited. Service accounts can be\\ncreated freely, and private keys for service accounts are not automatically rotated.\\n\\nYou are constantly identifying sensitive data, defined as data containing personally\\nidentifiable information (PII), in order to govern its protection, and the user’s privacy.\\n\\nBridging Data and Process, “Secure” concerns the extent to which you\\nunderstand and protect your data and ML services from unauthorized and\\ninappropriate access, in addition to ensuring responsible and explainable AI.\\n\\nData classification and protection\\nPoor data protection and sensitive data handling are big blockers for AI adoption, as they can\\nresult in breaches or other issues and, consequently, reputational damage or regulatory\\nsanctions. Data classification and loss prevention, for example, using Cloud Data Loss\\nPrevention, can help you identify and handle sensitive data through encryption, removing,\\nmasking, or coarsening. In addition, establishing a governance policy and adhering to regulatory\\ncompliance are important considerations at this phase. Another key aspect is to track data\\nlineage and its chain of custody, to ensure that the ML models informing business decisions are\\nbased on untampered data.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/18', 'id': '18', 'content': '16\\n\\nCenter of Excellence (COE),81 where engineers and data scientists with domain expertise work\\nside by side, drawing on business subject matter experts, as required by the use case. The\\ncentralized team aims to achieve several strategic goals:\\n• To ensure consistent standards and governance\\n• To assess the feasibility of new use cases\\n• To weigh in on new data collection\\n• To remain current on which products can be bought rather than built\\n• To help build ML solutions for different product teams\\n• To prioritize scarce ML resources around key business problems\\nAs the surrounding organizational fabric matures (for example, increased awareness of AI\\ncapabilities, increasing demand for utilizing those capabilities, increased consistency of\\ntooling and approach), it is more feasible for ML to be used independently in different\\nproducts and business areas.\\nAt this phase, teams have skills in data wrangling and descriptive and predictive analytics;\\nthey use existing frameworks, methods, and techniques to solve a variety of use cases; and\\nthey often deploy custom ML models in production. Teams retrieve their information from a\\nsingle source: an enterprise data warehouse (EDW), with complete, consistent, correct, and\\nconcurrent data. Extract, transform, load (ETL) and extract, load, transform (ELT) routines are\\nautomated and scalable.\\n\\nOpportunities for growth and advancement\\nAt this phase, organizations can benefit substantially from developing an AI capability that is\\nmore tailored to their business model and their distinct business needs. And from adding\\nautomated processes to their customized models.\\nTechnically, the next key moves are about protecting data quality, preventing ML models from\\ngoing stale, and enabling valuable solutions to be production ready. In addition, common\\npractices and guidelines are established for building secure, ethics-complaint AI solutions.\\n\\n8 No two Cloud COEs are quite the same. Data science skills are an important part of the picture. However, as the\\nlevel of maturity deepens and the focus of the Cloud COE evolves, it can make increasing sense to establish a distinct\\nadvanced analytics capability. You can read more about what makes an effective Cloud COE in this whitepaper.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/36', 'id': '36', 'content': '34\\n\\nTransformational maturity\\n\\nYour goal is to develop an ML engineering culture and practice that unifies ML system\\ndevelopment (ML) and ML system operations (Ops). MLOps strongly advocates automation\\nand monitoring at all steps of ML system construction — from integration, testing, and\\nreleasing to deployment, model serving, and infrastructure management. With this approach,\\nyou are working towards shorter development cycles, increased deployment velocity, and\\nmore dependable releases that are also in close alignment with your business objectives.\\nThis is crucial because you have a large number of models in production that require frequent\\nupdating both with new data to capture the emerging patterns and with the new\\nimplementation of state-of-the-art ML ideas.\\n\\nAutomatic detection of skews and anomalies in the data through regular data validation jobs\\nhelp data scientists to monitor the performance of their model. In addition, you have an\\nautomated A/B testing system to evaluate the effectiveness of a newly released model\\nservice.\\n\\nContinuous integration and delivery\\nIn ML, continuous integration (CI) covers testing and validating code and components, as well\\nas testing and validating data, data schemas, and models. Continuous delivery (CD) covers two\\naspects: 1) deploying the CT pipeline, which produces a newly trained (and validated) model\\nevery time it is executed, and 2) deploying the trained model as a prediction service to the\\nserving infrastructure. Services like Cloud Build, Container Registry, Model Registry, and ML\\nMetadata are required to streamline the CI/CD/CT of production ML systems, while maintaining\\nreproducibility, resumability, and reliability.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/29', 'id': '29', 'content': '27\\n\\nScale\\n\\nAn organization’s maturity in the Scale theme reflects that organization’s ability to scale data\\nprocessing and ML workloads.\\n\\nTactical maturity\\nTo control the cost of exploring and experimenting with ML and predictive analytics, you’ve\\ngot dedicated hardware or cloud compute instances for a few data scientists. Both the size\\nand the lifespan of these compute instances are dictated by the IT operations team. In\\naddition, your team of data scientists work with small, offline datasets for exploratory data\\nanalysis and processing, using simple data wrangling tools and ready-to-use AI services.\\nData processing and ML training are done locally, using the dedicated data science VMs,\\nwhich means limited compute power. You are actively exploring the potential of AI by finding\\nsimple, yet interesting, use cases that demonstrate proof of value (POV), so as to increase\\nthe awareness across the organization of the promise of advanced analytics.\\nStrategic maturity\\nYou’ve enabled an enterprise-wide, fully managed cloud data warehouse for ad hoc analytical\\nqueries, for example, using BigQuery. Data is now ingested into your cloud data warehouse\\nfrom various sources spread across the organization in different systems. Such a scalable\\ndata warehouse enables the data scientists across your organization to perform complex\\nanalytics and information retrieval on a large amount of data in a timely fashion.\\nData ingestion and transformation are performed using a serverless, fault-tolerant,\\nautoscaling, parallel processing service for handling large batch and streaming ETL/ELT\\nworkloads to populate the data warehouse and prepare ML datasets. You use tools such as\\nDataflow, Dataproc, and Cloud Data Fusion to combine and process both structured and\\nunstructured data at scale so as to gain actionable insights.\\n\\nBridging Data and Technology, “Scale” concerns the extent to which you use\\ncloud-native ML services that scale with large amounts of data and large\\nnumbers of data processing and ML jobs, with reduced operational overhead.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0.1]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/32', 'id': '32', 'content': '30\\n\\nStrategic maturity\\n\\nYour Cloud IAM policies reference a much more granular set of predefined roles, rather than\\nthe coarse primitive roles. In addition, your AI capabilities are supported by clear governance\\nand decision-making responsibilities. Building on this foundation, you’ve also provided a\\nverification route for decisions guided by the principles of AI,121 ensuring that the desired level\\nof trust in AI is maintained throughout the organization. This impacts how confident your\\norganization is in relying on AI to influence decision-making, while avoiding potential biases\\nin human-centric132 use cases.\\n\\nTransformational maturity\\n\\nYou aim to have a comprehensive understanding of the contents of all your data stores, so as\\nto obtain the threat profiles necessary for designing more effective security and data\\ngovernance models, models that consider scenarios of both unauthorized and inappropriate\\naccess. All Cloud Admin activity and data access logs are regularly audited, while automatic\\nalerts have been configured to watch for patterns that match your threat profiles. Cloud IAM\\npermissions and firewall rules are continuously monitored and corrected.\\n\\n12 We recognize that technologies that solve important problems also raise important challenges that we need\\nto address clearly, thoughtfully, and affirmatively. Artificial Intelligence at Google: Our Principles sets out our\\ncommitment to develop technology responsibly and establishes specific application areas we will not pursue.\\n13 People + AI Research (PAIR) is a multidisciplinary research and development team at Google that explores the\\nhuman side of AI by working with diverse communities. PAIR released a guidebook to help user experience (UX)\\nprofessionals and product managers follow a human-centered approach to AI.\\n\\nExplainable AI\\n\\nExplainable AI methods and techniques render AI solutions and outputs intelligible to human\\nexperts. This approach mitigates the concept of “blackboxing” in ML, where it is hard to explain\\nspecific decisions from an ML model. Your AI-enabled business may impact, or even redefine,\\nmany areas of society. The usefulness and fairness of these AI systems will be gated both by\\ntheir transparency and by your ability to understand, explain, and control them. Activating the\\nright Google tools and capabilities, such as What-If tool, Fairness Indicators, and Explainable AI,\\nwill not only speed up and secure the AI journey, it will also enable your organization to stay\\ncompliant with current regulations, and to react quickly when they change.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/6', 'id': '6', 'content': '4\\n\\nThe power of AI\\n\\nNew ideas are brought to market daily — some from established companies equipped with\\nindustry experience and capital, some from new companies armed with new technologies\\nand a desire to disrupt. What often bridges the gap between plans and outcomes is a\\ncompany’s ability to effectively make data-driven decisions and execute at scale.\\nThis is precisely what artificial intelligence with machine learning (ML)31 can do for the\\ncompanies that know how to take that information and use it well. Machine learning is\\nparticularly adept at finding patterns in complex datasets to solve complex problems,\\nincluding perceptual tasks, such as visual perception and speech recognition. The use cases\\nare both wide reaching and dynamic. Manufacturers, for example, are streamlining their\\ncapital expenses by implementing predictive maintenance. Financial institutions are\\nenhancing their risk analysis. Retailers and media providers are personalizing their customer\\nexperience. And the travel industry is offering their customers dynamic pricing predictions.\\n\\nAt the same time, academic and industrial advances in AI have resulted in better tooling,\\nsmarter algorithms, and more effective implementation techniques covering a wide range of\\nuse cases and datasets. These advances — combined with exponential gains in the cost of\\ndata storage, compute power, AI-centric hardware, and cloud computing — have\\ndemocratized AI for industry in an unprecedented way.\\nAI and ML are increasingly implicated in companies gaining a competitive edge, with direct\\nand attributable business value. In a research study we conducted in partnership with MIT\\nTechnology Review,42 we found that the adoption of ML results in 2x more data-driven\\ndecisions, 5x faster decision-making, and 3x faster execution. Enterprises that invest in\\nbuilding industry-specific AI solutions are proven to be better positioned as future global\\neconomic leaders. By 2030, companies that fully absorb AI could double their cash flow.53\\nMachine learning truly is reshaping the marketplace.\\n\\n3 Artificial intelligence is the theory and development of systems able to perform tasks normally requiring human\\nintelligence, such as visual perception, speech recognition, and decision-making. Machine learning is an effective way\\nfor building AI systems through automatically discovering useful patterns from data, rather than feeding human-writ\\nten rules to the system.\\n4 Machine Learning: The New Proving Ground for Competitive Advantage.\\n5 Notes from the AI frontier: Modeling the impact of AI on the world economy, McKinsey & Company, September\\n2018.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/35', 'id': '35', 'content': \"33\\n\\nYou are using ML models in production. However, the models often break when deployed in\\nthe real world because they fail to adapt to changes in the dynamics of the environment or to\\nchanges in the data that describes the environment. For this reason, to regularly update your\\nmodels with new data, you deploy end-to-end continuous training151 ML pipelines.\\nIn addition, your trained models are integrated with your automated ETL/ELT routines to\\nperform batch scoring. For online use cases, your models are deployed as services to be\\nused in apps and stream-processing pipelines for serving real-time predictions.\\n\\n15 Continuous training pipelines are automatically executed through schedules or event-based triggers that you can\\nset up with tools like Cloud Scheduler or Cloud Functions. You can orchestrate your training workflows using AI\\nPlatform Pipelines.\\n\\nContinuous training\\nFor reliable continuous training (CT) automation, the validation and quality control aspects of\\nyour ML training need to be backed into your pipelines. That is why you need data validation and\\nmodel validation steps in the beginning and the end of the pipeline, respectively. Such validation\\nsteps act as the “gatekeepers” to your model training. Data validation makes sure that the\\nschema and data types of the new data for retraining the model are as expected. Model\\nvalidation makes sure that the produced model meets the required predictive performance for\\ndeployment, for example, that it outperforms the current model in production and that it meets\\nthe fairness measures — if any have been specified.\\n\\nQuality control in machine learning\\n\\nWhen a deployed ML model produces bad predictions, the poor ML quality may imply a wide\\nrange of problems, including the presence of the bugs typical of any program, but also data\\nskews and anomalies, and the absence of proper procedures for evaluating models after\\ntraining and for validating models before deployment. Testing is required not only for\\ndevelopment, but for deployment and production as well. Instrumentations like logging,\\nmonitoring, and notifications are critical to maintain the system's health and operate it reliably.\\nSee Testing and Debugging in Machine Learning and What’s your ML test score? A rubric for ML\\nproduction systems, by Google’s ML experts.\", 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/17', 'id': '17', 'content': '15\\n\\nOrganizations at this phase should look to develop the foundational skill set for core data\\nwrangling and descriptive analytics. To drive effective collaboration across an organization\\nand to innovate with data from many sources, a key next move is to start bringing together\\nsiloed data from the many lines of business into a central, unified data lake, but with\\ndecentralized access. With a central data lake, it’s easier to derive insights from unstructured\\ndata and easier also to perform batch integration for reporting. Data is available through\\ncentralized tooling, but teams do not lose autonomy of access management or data\\nownership.\\nOrganizations at this phase can benefit, too, from the move toward use cases that are\\nfeasible and designed to deliver business value, but that begin to tackle increasingly complex\\nproblems.\\nAnd the quality of the analytics solutions improves when the organization focuses on\\ndeveloping core standards:\\n• A set of unified standards and technical practices that ensures the security of all data\\naccess and protection\\n• A set of common principles and procedures that prevents building any ML that may harm\\nyour brand (such as, for example, inadvertently building socially biased models)\\n\\nStrategic\\n\\nCharacteristics\\n\\nOrganizations at the strategic phase are focused on delivering sustainable business value,\\nwith several ML systems deployed and maintained in production that leverage both ready-to\\nuse and custom models. ML is no longer seen as the domain of a select few, but is instead in\\nthe process of becoming a pivotal accelerator for the business.\\nAt this phase, organizations typically require a degree of centralized coordination and so they\\nwill often create an advanced analytics71 team with the right skill set to build solutions for\\nvarious ML use cases across business functions. This team can be part of a broader Cloud\\n\\n7 A centralized team, spun out from the core Cloud COE. This team should have engineering skills in data, analytics,\\nand ML; and they should operate as the hub in a hub-and-spoke pattern, working closely with other ML and data\\nscience teams in the various lines of business.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/19', 'id': '19', 'content': '17\\n\\nOrganizationally, the centralized advanced analytics team with technical ML skills starts to\\nwork smoothly in conjunction with various functional teams with specialized domain\\nexpertise.\\n\\nTransformational\\n\\nCharacteristics\\n\\nOrganizations at the transformational phase are actively using AI to stimulate innovation, to\\nsupport agility, and to cultivate a culture where experimentation and learning is ongoing.\\nModels are built and deployed from a unified ML platform, making ML accessible to everyone\\nin the organization.\\nAt this phase, organizations typically model a hybrid approach to AI, with functional or\\nproduct-specific AI teams embedded into the broader product teams — supported by the\\nadvanced analytics team, which might become its own hub or center of excellence. The\\ncentral team enables a mechanism for scaling and cultivating these capabilities across the\\norganization: for example, implementing a mechanism for secondment or talent rotation,\\nwhereby business or product experts are immersed in ML techniques to learn hands-on. Over\\ntime, ML expertise diffuses across lines of business. The role of the centralized advanced\\nanalytics team becomes more confined to establishing common patterns and best practices\\nand to providing standard tools and libraries for accelerating ML projects. By contrast, the\\ndata science teams embedded in product groups or lines of business are responsible for\\nbuilding their function-specific ML models. This division of responsibility drives consistency\\nin building high-quality, technical solutions with real business impact.\\nAll teams are empowered through a platform that enables access to useful datasets,\\nprepared features, reusable components, and trained models. That platform is supported by\\nscalable and serverless compute for batch and online data ingestion and processing,\\ndistributed ML training and serving, access to databases with specialized storage and\\nquerying capabilities, and hardware accelerators.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/20', 'id': '20', 'content': '18\\n\\nOpportunities for growth and advancement\\n\\nAt this phase, organizations can benefit substantially from focusing on best practices,\\nensuring that AI practices are responsible, that they are based on sound principles, and that\\nAI systems are safe and robust.\\n\\nThe ML platform is supported with tools for continuous integration, continuous training, and\\ncontinuous model serving and monitoring. Building and maintaining such a platform is a\\nshared responsibility between ML and software engineers with skills in infrastructure,\\nDevOps, and SRE.\\n\\nOrganizations in this phase often assemble teams to conduct cutting-edge research and\\npresent at academic conferences and AI events. To do this, they focus their research and\\ninnovation in areas where they have unique capabilities, either in terms of domain\\nunderstanding or data availability, so that they can build a sustainable advantage over time.\\n\\nBesides the competitive edge that such a sustainable advantage provides, the in-house\\nresearch capabilities can add to an organization’s employee value proposition, becoming a\\npoint of differentiation to attract the best talent.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/16', 'id': '16', 'content': '14\\n\\nThe AI maturity phases\\n\\nThere are three natural phases to AI maturity: tactical, strategic, and transformational. Each\\norganization’s current approaches to AI will fall within one of these three phases. Each phase\\noffers opportunities for further exploration and development.\\n\\nTactical\\n\\nCharacteristics\\n\\nOrganizations at the tactical phase are exploring the potential of AI to deliver in the short\\nterm. Use cases tend to be narrow, and developers are typically leveraging exploratory data\\nanalysis (EDA) tools and ready-to-use AI and ML services for proofs of concept and\\nprototyping, for example, using a prebuilt computer vision service to detect printed and\\nhandwritten text, or using descriptive analytics to create a customer segmentation model.\\nAt this phase, organizations are aware of the promise of advanced analytics,61 but ML can be\\nseen as unattainable, and for this reason, complex problems are outsourced. Since the ML\\nprojects that exist do so through individual efforts, those projects might not be aligned with\\nthe organization’s business goals, and so even the most successfully deployed ML projects\\nmay have only limited business impact.\\nAt this phase, too, there may be no process to scale solutions consistency, nor the skill set to\\nsolve complex analytics problems.\\nOpportunities for growth and advancement\\nAt this phase, organizations can benefit substantially from better access to data. Integrated,\\ncleaner, and fresher data leads, in turn, to actionable insights better tailored to your needs,\\nwhich translates to sharper and more informed decision-making. Such swift improvement is\\nimmediately validating, and can help to paint a picture for stakeholders of the power of ML.\\n\\n6 Broadly, we think of advanced analytics as the ability to use data science and machine learning to create insight for\\nmore effective decision-making.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/9', 'id': '9', 'content': '7\\n\\nScale concerns the extent to which you use cloud-native\\nML services that scale with large amounts of data and\\nlarge numbers of data processing and ML jobs, with\\nreduced operational overhead. How are cloud-based\\nservices provisioned? Are they on demand or long\\nliving? How is capacity for workloads allocated?\\n\\nSecure concerns the extent to which you understand\\nand protect your data and ML services from\\nunauthorized and inappropriate access, in addition to\\nensuring responsible and explainable AI. What controls\\nare in place? What strategies govern the whole? How\\ndoes an organization establish trust in its AI capabilities\\nso that it is leveraged to drive business value?\\n\\nAutomate concerns the extent to which you are able to\\ndeploy, execute, and operate technology for data\\nprocessing and ML pipelines in production efficiently,\\nfrequently, and reliably. What triggers a process? How\\ndo you track data lineage? Are your pipelines fault\\ntolerant and resumable? How do you manage logging,\\nmonitoring, and notifications?', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D2/chunks/33', 'id': '33', 'content': '31\\n\\nYour data governance is streamlined, for example, by using automated workflows to rapidly\\nvalidate new use cases against your AI principles — allowing greater focus and discussion\\ntime on the edge cases. A team specialized in AI safety and robustness works to improve the\\nreliability and generalizability of ML models, recognizing the importance of well-calibrated\\nuncertainty and protecting against adversarial attacks.141\\n\\n14 Adversarial attacks refers to how ML models can be vulnerable to inputs maliciously constructed by adversaries to\\nforce misclassification.', 'documentMetadata': {'structData': {'restrictions': ['257f33442c74c67017fb982b6582c684']}}}}], 'totalSize': 34, 'attributionToken': '3AL0WwEKDAj42tW_BhDh1pecAxIkNjdmNzExNGYtMDAwMC0yNjlhLWFhMGMtMTQyMjNiYzFjZTIyIgdHRU5FUklDKnyb2NMwgZX3MO_B4zDC8J4VtY7pMJWSxTDy2e0wmNa3LfHN_TDPyIgx25iGMdS_4jCQ97Iw1am3MKOAlyL0zf0w1LKdFf6U9zDPv-IwtreMLezB4zDSyIgxntjTMLiO6TCb1rct9dntMNiYhjGOvp0V2Km3MI6RyTDFy_MXMAFKEjB4NmRhZTQ0MGM3NjU4MDU5M1KIAXByb2plY3RzLzEwNzc2NDk1OTkwODEvbG9jYXRpb25zL2dsb2JhbC9jb2xsZWN0aW9ucy9kZWZhdWx0X2NvbGxlY3Rpb24vZW5naW5lcy90ZXN0LWRlbG9pdHRlXzE2OTcwNTA1NTE2OTEvc2VydmluZ0NvbmZpZ3MvZGVmYXVsdF9zZWFyY2g', 'guidedSearchResult': {}, 'summary': {}, 'queryExpansionInfo': {'expandedQuery': True}}\n",
            "retrieved = ['D1', 'D2']\n"
          ]
        }
      ],
      "source": [
        "print('\\nstep #2')\n",
        "search_filter = get_filter(Agent('coordinator').read_restrictions)\n",
        "assert search_filter\n",
        "print(f'search_filter = {search_filter}')\n",
        "search_results = search('what is hugo?', search_filter)\n",
        "print(f'search_results = {search_results}')\n",
        "retrieved = retrieved_documents(search_results)\n",
        "print(f'retrieved = {retrieved}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "bb4022e3-1a0d-4876-b2e9-cf015ad4eaff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "bb4022e3-1a0d-4876-b2e9-cf015ad4eaff",
        "outputId": "f60b1728-e5d1-4e60-fb4b-43a9d4874dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "step #3\n",
            "{'agent_to_filter': {'coordinator': 'restrictions:ANY(\"f9b857ba14374d8c5f7fe05067f6d210\")'}}\n",
            "coordinator\n",
            "redirecting to a different agent, calling the tools or finishing...\n",
            "agent to filter from search_tool = {'coordinator': 'restrictions:ANY(\"f9b857ba14374d8c5f7fe05067f6d210\")'}\n",
            "search_filter from search_tool = restrictions:ANY(\"f9b857ba14374d8c5f7fe05067f6d210\")\n",
            "query from search_tool = embedding in generative AI\n",
            "search_response from search tool = {'results': [{'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/3', 'id': '3', 'content': \"Design of Adapter Tuning on Vertex AI\\n\\nOur Parameter Efficient Fine Tuning jobs run on Cloud TPUs/GPUs through Vertex AI’s training\\nservice. Vertex AI creates a separate tenant project for each customer project and runs the training\\nworkloads on Compute Engine VMs on the tenant project.\\nDuring the Parameter Efficient Fine Tuning process, the associated VMs in the tenant project load\\nthe frozen model weights and the training dataset that consists of hundreds of prompt/response\\nexamples. The adapter weights are trained by using gradient-based optimization methods, and\\nthen checkpointed to a per-customer bucket in the tenant project during the training. Once the\\ntraining is done, the final adapter weights are stored in the same per-customer bucket for\\ndeployment.\\n\\nSecurity considerations\\nThroughout the training process, user input data remains confined to the user's storage bucket in\\nthe customer project, ensuring no external data transfer. Data can be encrypted by using\\n\\n3\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0.1]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/1', 'id': '1', 'content': 'Adaptation of Large Foundation Models\\n\\nAbout This Whitepaper\\n\\nTuning a large model involves adjusting and adapting a pre-trained model to perform specific tasks\\nwith the user provided training dataset. This white paper is a technical reference aimed at outlining\\nGoogle’s approach on adapter tuning. Developers who are responsible for deploying machine\\nlearning models, product managers, business leaders, and security engineers will find parts of this\\nwhite paper relevant, particularly around how data is handled in various stages and the current\\nlimitations. Implementation details are current as of July 11th, 2023 and are subject to change.\\n\\nOverview\\n\\nBackground on Foundation Models\\nFoundation models, also known as large-scale pre-trained models, have become a transformative\\nforce in the field of artificial intelligence (AI) and machine learning (ML). These models are\\ntypically trained on vast amounts of diverse data, allowing them to learn general patterns and\\nrepresentations that can be applied across various domains and tasks.\\nThe power of foundation models lies in their ability to learn high-quality, transferable features from\\nthe data they are trained on during the pre-training phase. This pre-training phase enables the\\nmodels to capture relationships and develop an understanding of the data, which can then be\\nfine-tuned to specific tasks. As a result, foundation models have proven to be highly effective in\\nnumerous applications, such as natural language processing, computer vision, and reinforcement\\nlearning.\\nThe widespread success of foundation models can be attributed to several factors, including:\\n● Large-scale data: Foundation models are trained on large datasets, which provide a source\\nof diverse information for learning general patterns and structures. This data exposure\\nallows the models to develop a nuanced understanding of a wide variety of domains and\\ntasks.\\n\\n● Advanced architecture: The architecture of foundation models, such as the Transformer,\\nenables them to capture complex relationships.\\n● Hardware advancements: The performance of foundation models tends to improve with\\nincreased size and computational resources. Researchers and organizations have been\\n\\n1', 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/4', 'id': '4', 'content': \"Customer-Managed Encryption Keys (CMEK) on Cloud Storage and accessed via the user-provided\\n\\n2\\n\\nservice account for enhanced security and inside the VPC security perimeter of the customer.\\nTemporary checkpoints generated in the tenant's project bucket are subject to automatic deletion\\nwithin a 30-day period, minimizing the risk of unauthorized access or unintended data retention.\\nAccess to the tenant project by anyone in Google other than the customer, requires a valid business\\njustification, such as a request for Google to assist in debugging an issue or a system outage. Any\\nrequest is logged through Access Transparency logs to maintain a comprehensive record of all\\ninteractions and safeguard against potential security breaches.\\n\\nDesign of Adapter serving on Vertex AI\\n\\nOnce the adapter layers are trained, the weights are uploaded as a model to a bucket in the Vertex\\nPrediction per-customer tenant project and is deployed to an Endpoint in the customer’s project.\\nWhen serving a request on an endpoint, the adapter weights are loaded from the bucket, cached to\\nminimize latency and sent to the foundation model along with the original request (e.g. user's\\nprompt) for inference. The latency overhead of transferring the adapter layers is typically negligible\\ncompared to the inference of a large model.\\nFoundation model service has the frozen model weights loaded during the start up. It receives the\\nadapter weights for the duration of an inference, runs through the request and returns the results,\\nwithout modifying the model or storing the request.\\n\\n2CMEK, User Provided Service accounts and VPC Service controls are currently not supported.\\n\\n4\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}], 'totalSize': 3, 'attributionToken': '3AL0WwEKDAiI29W_BhD8graRAhIkNjdmMjNkNGQtMDAwMC0yZGQxLWJkMDMtZDQzYTJjYzcyNTE3IgdHRU5FUklDKnzUsp0VkPeyMNWptzDUv-IwntjTMMXL8xf0zf0w_pT3MOzB4zDPv-Iw0siIMbiO6TD12e0wm9a3LdiYhjG3t4wtjr6dFdiptzCOkckwgZX3MJvY0zDC8J4Vo4CXIu_B4zDxzf0w25iGMc_IiDG1jukwmNa3LfLZ7TCVksUwMAFKEjB4MTU4NWQ0YTY1YjZmMzc0YVKIAXByb2plY3RzLzEwNzc2NDk1OTkwODEvbG9jYXRpb25zL2dsb2JhbC9jb2xsZWN0aW9ucy9kZWZhdWx0X2NvbGxlY3Rpb24vZW5naW5lcy90ZXN0LWRlbG9pdHRlXzE2OTcwNTA1NTE2OTEvc2VydmluZ0NvbmZpZ3MvZGVmYXVsdF9zZWFyY2g', 'guidedSearchResult': {}, 'summary': {}, 'queryExpansionInfo': {'expandedQuery': True}}\n",
            "{'results': [{'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/3', 'id': '3', 'content': \"Design of Adapter Tuning on Vertex AI\\n\\nOur Parameter Efficient Fine Tuning jobs run on Cloud TPUs/GPUs through Vertex AI’s training\\nservice. Vertex AI creates a separate tenant project for each customer project and runs the training\\nworkloads on Compute Engine VMs on the tenant project.\\nDuring the Parameter Efficient Fine Tuning process, the associated VMs in the tenant project load\\nthe frozen model weights and the training dataset that consists of hundreds of prompt/response\\nexamples. The adapter weights are trained by using gradient-based optimization methods, and\\nthen checkpointed to a per-customer bucket in the tenant project during the training. Once the\\ntraining is done, the final adapter weights are stored in the same per-customer bucket for\\ndeployment.\\n\\nSecurity considerations\\nThroughout the training process, user input data remains confined to the user's storage bucket in\\nthe customer project, ensuring no external data transfer. Data can be encrypted by using\\n\\n3\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0.1]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/1', 'id': '1', 'content': 'Adaptation of Large Foundation Models\\n\\nAbout This Whitepaper\\n\\nTuning a large model involves adjusting and adapting a pre-trained model to perform specific tasks\\nwith the user provided training dataset. This white paper is a technical reference aimed at outlining\\nGoogle’s approach on adapter tuning. Developers who are responsible for deploying machine\\nlearning models, product managers, business leaders, and security engineers will find parts of this\\nwhite paper relevant, particularly around how data is handled in various stages and the current\\nlimitations. Implementation details are current as of July 11th, 2023 and are subject to change.\\n\\nOverview\\n\\nBackground on Foundation Models\\nFoundation models, also known as large-scale pre-trained models, have become a transformative\\nforce in the field of artificial intelligence (AI) and machine learning (ML). These models are\\ntypically trained on vast amounts of diverse data, allowing them to learn general patterns and\\nrepresentations that can be applied across various domains and tasks.\\nThe power of foundation models lies in their ability to learn high-quality, transferable features from\\nthe data they are trained on during the pre-training phase. This pre-training phase enables the\\nmodels to capture relationships and develop an understanding of the data, which can then be\\nfine-tuned to specific tasks. As a result, foundation models have proven to be highly effective in\\nnumerous applications, such as natural language processing, computer vision, and reinforcement\\nlearning.\\nThe widespread success of foundation models can be attributed to several factors, including:\\n● Large-scale data: Foundation models are trained on large datasets, which provide a source\\nof diverse information for learning general patterns and structures. This data exposure\\nallows the models to develop a nuanced understanding of a wide variety of domains and\\ntasks.\\n\\n● Advanced architecture: The architecture of foundation models, such as the Transformer,\\nenables them to capture complex relationships.\\n● Hardware advancements: The performance of foundation models tends to improve with\\nincreased size and computational resources. Researchers and organizations have been\\n\\n1', 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}, {'modelScores': {'relevance_score': {'values': [0]}}, 'chunk': {'name': 'projects/1077649599081/locations/global/collections/default_collection/dataStores/test-subfolder_1697051023694/branches/0/documents/D1/chunks/4', 'id': '4', 'content': \"Customer-Managed Encryption Keys (CMEK) on Cloud Storage and accessed via the user-provided\\n\\n2\\n\\nservice account for enhanced security and inside the VPC security perimeter of the customer.\\nTemporary checkpoints generated in the tenant's project bucket are subject to automatic deletion\\nwithin a 30-day period, minimizing the risk of unauthorized access or unintended data retention.\\nAccess to the tenant project by anyone in Google other than the customer, requires a valid business\\njustification, such as a request for Google to assist in debugging an issue or a system outage. Any\\nrequest is logged through Access Transparency logs to maintain a comprehensive record of all\\ninteractions and safeguard against potential security breaches.\\n\\nDesign of Adapter serving on Vertex AI\\n\\nOnce the adapter layers are trained, the weights are uploaded as a model to a bucket in the Vertex\\nPrediction per-customer tenant project and is deployed to an Endpoint in the customer’s project.\\nWhen serving a request on an endpoint, the adapter weights are loaded from the bucket, cached to\\nminimize latency and sent to the foundation model along with the original request (e.g. user's\\nprompt) for inference. The latency overhead of transferring the adapter layers is typically negligible\\ncompared to the inference of a large model.\\nFoundation model service has the frozen model weights loaded during the start up. It receives the\\nadapter weights for the duration of an inference, runs through the request and returns the results,\\nwithout modifying the model or storing the request.\\n\\n2CMEK, User Provided Service accounts and VPC Service controls are currently not supported.\\n\\n4\", 'documentMetadata': {'structData': {'restrictions': ['f9b857ba14374d8c5f7fe05067f6d210']}}}}], 'totalSize': 3, 'attributionToken': '3AL0WwEKDAiI29W_BhD8graRAhIkNjdmMjNkNGQtMDAwMC0yZGQxLWJkMDMtZDQzYTJjYzcyNTE3IgdHRU5FUklDKnzUsp0VkPeyMNWptzDUv-IwntjTMMXL8xf0zf0w_pT3MOzB4zDPv-Iw0siIMbiO6TD12e0wm9a3LdiYhjG3t4wtjr6dFdiptzCOkckwgZX3MJvY0zDC8J4Vo4CXIu_B4zDxzf0w25iGMc_IiDG1jukwmNa3LfLZ7TCVksUwMAFKEjB4MTU4NWQ0YTY1YjZmMzc0YVKIAXByb2plY3RzLzEwNzc2NDk1OTkwODEvbG9jYXRpb25zL2dsb2JhbC9jb2xsZWN0aW9ucy9kZWZhdWx0X2NvbGxlY3Rpb24vZW5naW5lcy90ZXN0LWRlbG9pdHRlXzE2OTcwNTA1NTE2OTEvc2VydmluZ0NvbmZpZ3MvZGVmYXVsdF9zZWFyY2g', 'guidedSearchResult': {}, 'summary': {}, 'queryExpansionInfo': {'expandedQuery': True}}\n",
            "redirecting to the last agent...\n",
            "coordinator\n",
            "redirecting to a different agent, calling the tools or finishing...\n",
            "\n",
            "state = {'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'SystemMessage'], 'kwargs': {'content': 'You are a helpful assistant with access to a search tool', 'type': 'system', 'id': '9b0faa41-6fd7-4c64-b39c-e36bc102b2cc'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'what is an embedding related to generative ai?', 'type': 'human', 'id': '8618195a-7138-4299-932a-665bc7c69ab9'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'function_call': {'name': 'search_tool', 'arguments': '{\"query\": \"embedding in generative AI\"}'}}, 'response_metadata': {'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 38, 'candidates_token_count': 8, 'total_token_count': 46, 'prompt_tokens_details': [{'modality': 1, 'token_count': 38}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 8}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.14982354640960693, 'model_name': 'gemini-2.0-flash'}, 'type': 'ai', 'id': 'run-721a5cfe-848b-4e89-97ec-b68b89ededa8-0', 'tool_calls': [{'name': 'search_tool', 'args': {'query': 'embedding in generative AI'}, 'id': '90359255-8224-47fa-8904-c11ea4924116', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': [], 'type': 'tool', 'name': 'search_tool', 'id': '4494977a-7166-4465-b532-414f9b7980d1', 'tool_call_id': '90359255-8224-47fa-8904-c11ea4924116', 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'response_metadata': {'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 42, 'total_token_count': 42, 'prompt_tokens_details': [{'modality': 1, 'token_count': 42}], 'candidates_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': [], 'candidates_tokens_details': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, 'type': 'ai', 'id': 'run-3a4995de-bc28-44f5-9d7f-41d434392381-0', 'usage_metadata': {'input_tokens': 42, 'output_tokens': 0, 'total_tokens': 42}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'agent_history': ['coordinator', 'coordinator'], 'next_agent': None, 'retrieved_documents': [[]]}\n",
            "retrieved = []\n"
          ]
        }
      ],
      "source": [
        "# == step 3\n",
        "# User U1 asks question Q1.\n",
        "# Agent A1 uses the search tool.\n",
        "# Document D1 is present in the search results.\n",
        "# Document D2 is NOT present in the search results.\n",
        "question_Q1 = 'what is an embedding related to generative ai?'\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant with access to a search tool'},\n",
        "    {'role': 'user', 'content': question_Q1}\n",
        "]\n",
        "print('\\nstep #3')\n",
        "user_info = {\n",
        "    'agent_to_filter': {\n",
        "        agent_id: get_filter(reconcile_restrictions(User('U1').read_restrictions, agent.read_restrictions))\n",
        "        for agent_id, agent in Agent.all().items()\n",
        "    }\n",
        "}\n",
        "print(user_info)\n",
        "state = re_agent.query(input={'messages': messages}, config={'configurable': {'user_info': user_info}})\n",
        "print(final_response(state))\n",
        "\n",
        "print(f'state = {state}')\n",
        "\n",
        "retrieved = retrieved_documents(state)\n",
        "print(f'retrieved = {retrieved}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326360fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# == step 4\n",
        "# User U2 asks question Q1.\n",
        "# Agent A1 uses the search tool.\n",
        "# Document D2 is present in the search results.\n",
        "# Document D1 is NOT present in the search results.\n",
        "print('\\nstep #4')\n",
        "user_info = {\n",
        "    'agent_to_filter': {\n",
        "        agent_id: get_filter(reconcile_restrictions(User('U2').read_restrictions, agent.read_restrictions))\n",
        "        for agent_id, agent in Agent.all().items()\n",
        "    }\n",
        "}\n",
        "state = re_agent.query(input={'messages': messages}, config={'configurable': {'user_info': user_info}})\n",
        "print(final_response(state))\n",
        "print(state['agent_history'])\n",
        "retrieved = retrieved_documents(state)\n",
        "print(retrieved)\n",
        "assert 'ai_adoption_framework_whitepaper' in retrieved\n",
        "assert 'adaptation_of_foundation_models_whitepaper_google_cloud' not in retrieved\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c24693c-6ed7-45f6-9af3-a433ab86ec8b",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "2c24693c-6ed7-45f6-9af3-a433ab86ec8b"
      },
      "source": [
        "# Scenario 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621cd0e1-3b2c-414a-982f-6f06113a33ff",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "621cd0e1-3b2c-414a-982f-6f06113a33ff"
      },
      "outputs": [],
      "source": [
        "def scenario_2(re_app):\n",
        "    \"\"\"Users accessing documents based on their subscription level\"\"\"\n",
        "\n",
        "    print('scenario #2')\n",
        "    # == step 1\n",
        "    # User U1 is a member of company HVAC_Corp.\n",
        "    # User U1 has read access to resources belonging to HVAC_Corp.\n",
        "    # Document D1 is related to Basic subscription tier.\n",
        "    # Document D2 is related to Premium subscription tier.\n",
        "    # HVAC_Corp has access to resources related to Basic subscription.\n",
        "    # HVAC_Corp has access to resources related to Premium subscription.\n",
        "    # Agent A1 has read access to resources related to Basic subscription.\n",
        "    # Agent A2 has read access to resources related to Premium subscription.\n",
        "    # Document D1 is relevant for question Q1.\n",
        "    # Document D2 is relevant for question Q1.\n",
        "    # Document D1 is relevant for question Q2.\n",
        "    # Document D2 is relevant for question Q2.\n",
        "    print('\\nstep #1')\n",
        "    clear_environment()\n",
        "    show_environment()\n",
        "    Group('HVAC_Corp').restrict_read(Access(scope='company', specifier='HVAC_Corp'))\n",
        "    Group('HVAC_Corp').add_member(User('U1'))\n",
        "    documents = [\n",
        "        Document(\n",
        "            id='D1',\n",
        "            restrictions=[Restriction(Access(scope='subscription', specifier='Basic'))],\n",
        "            metadata={},\n",
        "            mime='application/pdf',\n",
        "            uri='gs://data_storage_aiml_hugo/hvac_basic_resources.pdf'\n",
        "        ),\n",
        "        Document(\n",
        "            id='D2',\n",
        "            restrictions=[Restriction(Access(scope='subscription', specifier='Premium'))],\n",
        "            metadata={},\n",
        "            mime='application/pdf',\n",
        "            uri='gs://data_storage_aiml_hugo/hvac_premium_resources.pdf'\n",
        "        )\n",
        "    ]\n",
        "    for doc in documents:\n",
        "        print(doc.create())\n",
        "        print(doc.wait_till_indexed())\n",
        "    Group('Basic_Tier').restrict_read(Access(scope='subscription', specifier='Basic'))\n",
        "    Group('Premium_Tier').restrict_read(Access(scope='subscription', specifier='Premium'))\n",
        "    Group('Basic_Tier').add_member(Group('HVAC_Corp'))\n",
        "    Group('Premium_Tier').add_member(Group('HVAC_Corp'))\n",
        "    agent_A1 = 'coordinator'\n",
        "    agent_A2 = 'past'\n",
        "    Group('Basic_Tier').add_member(Agent(agent_A1))\n",
        "    Group('Premium_Tier').add_member(Agent(agent_A2))\n",
        "    question_Q1 = 'what are best practices for HVAC installation?'\n",
        "    question_Q2 = 'what were the historical trends in HVAC technology?'\n",
        "    user_info = {\n",
        "        'agent_to_filter': {\n",
        "            agent_id: get_filter(reconcile_restrictions(User('U1').read_restrictions, agent.read_restrictions))\n",
        "            for agent_id, agent in Agent.all().items()\n",
        "        }\n",
        "    }\n",
        "    show_environment()\n",
        "\n",
        "    # == step 2\n",
        "    # Direct search attempt, using the permissions of User U1, retrieves all relevant documents for question Q1.\n",
        "    # Direct search attempt, using the permissions of User U1, retrieves all relevant documents for question Q2.\n",
        "    print('\\nstep #2')\n",
        "    search_filter = get_filter(User('U1').read_restrictions)\n",
        "    assert search_filter\n",
        "    search_results = search(question_Q1, search_filter)\n",
        "    retrieved = retrieved_documents(search_results)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_basic_resources' in retrieved\n",
        "    assert 'hvac_premium_resources' in retrieved\n",
        "    search_results = search(question_Q2, search_filter)\n",
        "    retrieved = retrieved_documents(search_results)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_basic_resources' in retrieved\n",
        "    assert 'hvac_premium_resources' in retrieved\n",
        "\n",
        "    # == step 3\n",
        "    # User U1 asks question Q1.\n",
        "    # Agent A1 uses the search tool.\n",
        "    # Document D1 is present in the search results.\n",
        "    # Document D2 is NOT present in the search results.\n",
        "    print('\\nstep #3')\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'You are a helpful assistant for HVAC professionals.'},\n",
        "        {'role': 'user', 'content': question_Q1}\n",
        "    ]\n",
        "    state = re_app.query(input={'messages': messages}, config={'configurable': {'user_info': user_info}})\n",
        "    print(final_response(state))\n",
        "    print(state['agent_history'])\n",
        "    retrieved = retrieved_documents(state)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_basic_resources' in retrieved\n",
        "    assert 'hvac_premium_resources' not in retrieved\n",
        "\n",
        "    # == step 4\n",
        "    # User U1 asks question Q2.\n",
        "    # Agent A2 uses the search tool.\n",
        "    # Document D1 is NOT present in the search results.\n",
        "    # Document D2 is present in the search results.\n",
        "    print('\\nstep #4')\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'You are a helpful assistant for HVAC professionals.'},\n",
        "        {'role': 'user', 'content': question_Q2}\n",
        "    ]\n",
        "    state = re_app.query(input={'messages': messages}, config={'configurable': {'user_info': user_info}})\n",
        "    print(final_response(state))\n",
        "    print(state['agent_history'])\n",
        "    retrieved = retrieved_documents(state)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_basic_resources' not in retrieved\n",
        "    assert 'hvac_premium_resources' in retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60ad1f6-ea15-4c5f-b106-36cb84b8d124",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "a60ad1f6-ea15-4c5f-b106-36cb84b8d124",
        "outputId": "f39b383c-2364-4b25-832a-0f08ac52a0fc"
      },
      "outputs": [],
      "source": [
        "scenario_2(remote_app)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ccc3bf-9690-47b6-9bdb-d5bbabc86eed",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "55ccc3bf-9690-47b6-9bdb-d5bbabc86eed"
      },
      "source": [
        "# Scenario 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e76e13a-da85-4593-a427-1ce32a798979",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "7e76e13a-da85-4593-a427-1ce32a798979"
      },
      "outputs": [],
      "source": [
        "def scenario_3(re_app):\n",
        "    \"\"\"Changing permissions for a user reflected in search results\"\"\"\n",
        "\n",
        "    print('scenario #3')\n",
        "    # == step 1\n",
        "    # User U1 is a member of company HVAC_Corp.\n",
        "    # User U1 has read access to resources belonging to HVAC_Corp.\n",
        "    # Document D1 belongs to HVAC_Corp's company documents.\n",
        "    # Document D2 is related to Premium feature documentation.\n",
        "    # Premium subscription tier has read access to Premium feature documentation.\n",
        "    # Document D1 is relevant for question Q1.\n",
        "    # Document D2 is relevant for question Q1.\n",
        "    # Agent A1 has read access to resources belonging to HVAC_Corp.\n",
        "    # Agent A1 has read access to resources related to Premium features.\n",
        "    print('\\nstep #1')\n",
        "    # clear_environment()\n",
        "    show_environment()\n",
        "    Group('HVAC_Corp').restrict_read(Access(scope='company', specifier='HVAC_Corp'))\n",
        "    Group('HVAC_Corp').add_member(User('U1'))\n",
        "    documents = [\n",
        "        Document(\n",
        "            id='D1',\n",
        "            restrictions=[Restriction(Access(scope='company', specifier='HVAC_Corp'))],\n",
        "            metadata={},\n",
        "            mime='application/pdf',\n",
        "            uri='gs://data_storage_aiml_hugo/hvac_company_manual.pdf'\n",
        "        ),\n",
        "        Document(\n",
        "            id='D2',\n",
        "            restrictions=[Restriction(Access(scope='subscription', specifier='Premium'))],\n",
        "            metadata={},\n",
        "            mime='application/pdf',\n",
        "            uri='gs://data_storage_aiml_hugo/hvac_premium_features.pdf'\n",
        "        )\n",
        "    ]\n",
        "    for doc in documents:\n",
        "        print(doc.create())\n",
        "        print(doc.wait_till_indexed())\n",
        "    Group('Premium_Tier').restrict_read(Access(scope='subscription', specifier='Premium'))\n",
        "    Group('Premium_Tier').add_member(Group('Premium_Subscribers'))\n",
        "    agent_A1 = 'future'\n",
        "    Group('HVAC_Corp').add_member(Agent(agent_A1))\n",
        "    Group('Premium_Tier').add_member(Agent(agent_A1))\n",
        "    question_Q1 = 'what are the upcoming HVAC technologies?'\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'You are a helpful assistant for HVAC professionals.'},\n",
        "        {'role': 'user', 'content': question_Q1}\n",
        "    ]\n",
        "    show_environment()\n",
        "\n",
        "    # == step 2\n",
        "    # Direct search attempt, using the permissions of Agent A1, retrieves all relevant documents for question Q1.\n",
        "    print('\\nstep #2')\n",
        "    search_filter = get_filter(Agent(agent_A1).read_restrictions)\n",
        "    assert search_filter\n",
        "    search_results = search(question_Q1, search_filter)\n",
        "    retrieved = retrieved_documents(search_results)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_company_manual' in retrieved\n",
        "    assert 'hvac_premium_features' in retrieved\n",
        "\n",
        "    # == step 3\n",
        "    # User U1 asks question Q1.\n",
        "    # Agent A1 uses the search tool.\n",
        "    # Document D1 is present in the search results.\n",
        "    # Document D2 is NOT present in the search results.\n",
        "    print('\\nstep #3')\n",
        "    user_info = {\n",
        "        'agent_to_filter': {\n",
        "            agent_id: get_filter(reconcile_restrictions(User('U1').read_restrictions, agent.read_restrictions))\n",
        "            for agent_id, agent in Agent.all().items()\n",
        "        }\n",
        "    }\n",
        "    state = re_app.query(input={'messages': messages}, config={'configurable': {'user_info': user_info}})\n",
        "    print(final_response(state))\n",
        "    print(state['agent_history'])\n",
        "    retrieved = retrieved_documents(state)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_company_manual' in retrieved\n",
        "    assert 'hvac_premium_features' not in retrieved\n",
        "\n",
        "    # == step 4\n",
        "    # HVAC_Corp upgrades to Premium subscription tier.\n",
        "    # User U1 asks question Q1.\n",
        "    # Agent A1 uses the search tool.\n",
        "    # Document D1 is present in the search results.\n",
        "    # Document D2 is present in the search results.\n",
        "    print('\\nstep #4')\n",
        "    Group('Premium_Subscribers').add_member(Group('HVAC_Corp'))\n",
        "    user_info = {\n",
        "        'agent_to_filter': {\n",
        "            agent_id: get_filter(reconcile_restrictions(User('U1').read_restrictions, agent.read_restrictions))\n",
        "            for agent_id, agent in Agent.all().items()\n",
        "        }\n",
        "    }\n",
        "    state = re_app.query(input={'messages': messages}, config={'configurable': {'user_info': user_info}})\n",
        "    print(final_response(state))\n",
        "    print(state['agent_history'])\n",
        "    retrieved = retrieved_documents(state)\n",
        "    print(retrieved)\n",
        "    assert 'hvac_company_manual' in retrieved\n",
        "    assert 'hvac_premium_features' in retrieved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d9b260-0de5-486d-bcb4-28651a0b1536",
      "metadata": {
        "deletable": true,
        "editable": true,
        "frozen": false,
        "id": "a0d9b260-0de5-486d-bcb4-28651a0b1536",
        "outputId": "8559edf5-3849-4a3a-d353-5de6c1824ea5"
      },
      "outputs": [],
      "source": [
        "scenario_3(remote_app)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0181adb7-0422-4a27-9fd4-7f2a6df39e77",
      "metadata": {
        "id": "0181adb7-0422-4a27-9fd4-7f2a6df39e77"
      },
      "source": [
        "# DB design matching the class design"
      ]
    },
    {
      "cell_type": "raw",
      "id": "6a88ba4e-c37a-47c4-97fe-5eb36eec5c2c",
      "metadata": {
        "id": "6a88ba4e-c37a-47c4-97fe-5eb36eec5c2c"
      },
      "source": [
        "Access:\n",
        "- id\n",
        "- scope\n",
        "- specifier\n",
        "\n",
        "Restriction:\n",
        "- id\n",
        "\n",
        "RestrictionHasAccess:\n",
        "- restriction_id\n",
        "- access_id\n",
        "\n",
        "Principal:\n",
        "- id\n",
        "- read_restriction (restriction_id)\n",
        "- write_restriction (restriction_id)\n",
        "- is_group (true | false)\n",
        "\n",
        "PrincipalIsMemberOfGroup\n",
        "- principal_id\n",
        "- group_id (principal_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cfc4229-5e17-4b49-afca-7eef59011687",
      "metadata": {
        "id": "8cfc4229-5e17-4b49-afca-7eef59011687"
      },
      "source": [
        "# Alternative DB design (without groups)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4f3a750f-a629-411b-a02f-5f2599c0afda",
      "metadata": {
        "id": "4f3a750f-a629-411b-a02f-5f2599c0afda"
      },
      "source": [
        "Access:\n",
        "- id\n",
        "- scope\n",
        "- specifier\n",
        "\n",
        "Restriction:\n",
        "- id\n",
        "\n",
        "RestrictionHasAccess:\n",
        "- restriction_id\n",
        "- access_id\n",
        "\n",
        "Principal:\n",
        "- id\n",
        "\n",
        "PrincipalHasRestriction:\n",
        "- principal_id\n",
        "- restriction_id\n",
        "- restriction_type (read | write)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df3a04c-2f95-4daf-ac84-8189a9642760",
      "metadata": {
        "id": "5df3a04c-2f95-4daf-ac84-8189a9642760"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_nr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
